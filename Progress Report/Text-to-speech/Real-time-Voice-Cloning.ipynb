{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FYP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ddOwidl3wUY",
        "outputId": "a7344b24-093e-4740-a760-6384e8165552"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-O7-hyn4AKp",
        "outputId": "46e04699-2548-4031-f56b-f0ea050317cd"
      },
      "source": [
        "cd gdrive/MyDrive/Real-Time-Voice-Cloning/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Real-Time-Voice-Cloning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns6T_vzdyYUG",
        "outputId": "4c0f4074-367b-4db3-b2ef-6e5f44629690"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdatasets_root\u001b[0m/             \u001b[01;34msamples\u001b[0m/\n",
            "demo_cli.py                \u001b[01;34msynthesizer\u001b[0m/\n",
            "demo_toolbox_collab.ipynb  synthesizer_preprocess_audio.py\n",
            "demo_toolbox.py            synthesizer_preprocess_embeds.py\n",
            "\u001b[01;34mencoder\u001b[0m/                   synthesizer_train.py\n",
            "encoder_preprocess.py      \u001b[01;34mtoolbox\u001b[0m/\n",
            "encoder_train.py           \u001b[01;34mutils\u001b[0m/\n",
            "LICENSE.txt                \u001b[01;34mvocoder\u001b[0m/\n",
            "README.md                  vocoder_preprocess.py\n",
            "requirements_gpu.txt       vocoder_train.py\n",
            "requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrE03Wq-4Y7N",
        "outputId": "06ad035c-3be6-4cdd-9cdc-e07e48f0a273"
      },
      "source": [
        "!pip install --upgrade pip\r\n",
        "!pip install torch\r\n",
        "!pip install -r requirements.txt\r\n",
        "!pip install webrtcvad"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (21.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Ignoring numpy: markers 'platform_system == \"Windows\"' don't match your environment\n",
            "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: visdom in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.1.8.9)\n",
            "Requirement already satisfied: librosa>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.8.0)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (3.3.4)\n",
            "Requirement already satisfied: numpy==1.19.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.19.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (4.41.1)\n",
            "Requirement already satisfied: sounddevice in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.4.1)\n",
            "Requirement already satisfied: SoundFile in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (0.10.3.post1)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (1.2.0)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (2.1.0)\n",
            "Requirement already satisfied: PyQt5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (5.15.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (0.70.11.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (0.51.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (1.32.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (1.15.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.8.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.8.0->-r requirements.txt (line 4)) (0.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.8.0->-r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.8.0->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.8.0->-r requirements.txt (line 4)) (0.22.2.post1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.8.0->-r requirements.txt (line 4)) (2.1.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.3.0->-r requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.3.0->-r requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.3.0->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.3.0->-r requirements.txt (line 5)) (7.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.3.0->-r requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->-r requirements.txt (line 16)) (53.0.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.6/dist-packages (from numba->-r requirements.txt (line 16)) (0.34.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from SoundFile->-r requirements.txt (line 11)) (1.14.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->SoundFile->-r requirements.txt (line 11)) (2.20)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15->-r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pooch>=1.0->librosa>=0.8.0->-r requirements.txt (line 4)) (20.9)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.6/dist-packages (from pooch>=1.0->librosa>=0.8.0->-r requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pooch>=1.0->librosa>=0.8.0->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r requirements.txt (line 1)) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: dill>=0.3.3 in /usr/local/lib/python3.6/dist-packages (from multiprocess->-r requirements.txt (line 15)) (0.3.3)\n",
            "Requirement already satisfied: PyQt5-sip<13,>=12.8 in /usr/local/lib/python3.6/dist-packages (from PyQt5->-r requirements.txt (line 14)) (12.8.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.6/dist-packages (from umap-learn->-r requirements.txt (line 2)) (0.5.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (0.57.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (1.28)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (22.0.2)\n",
            "Requirement already satisfied: torchfile in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (0.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.6/dist-packages (from jsonpatch->visdom->-r requirements.txt (line 3)) (2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pooch>=1.0->librosa>=0.8.0->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pooch>=1.0->librosa>=0.8.0->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pooch>=1.0->librosa>=0.8.0->-r requirements.txt (line 4)) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pooch>=1.0->librosa>=0.8.0->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: webrtcvad in /usr/local/lib/python3.6/dist-packages (2.0.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJZU40Hvo1BJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7187e733-6c1b-418b-a2ca-9fbaf2ae1d27"
      },
      "source": [
        "!python synthesizer_preprocess_audio.py datasets_root --datasets_name LibriSpeech --subfolders obama-10-utterance --no_alignments"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arguments:\n",
            "    datasets_root:   datasets_root\n",
            "    out_dir:         datasets_root/SV2TTS/synthesizer\n",
            "    n_processes:     None\n",
            "    skip_existing:   False\n",
            "    hparams:         \n",
            "    no_alignments:   True\n",
            "    datasets_name:   LibriSpeech\n",
            "    subfolders:      obama-10-utterance\n",
            "\n",
            "Using data from:\n",
            "    datasets_root/LibriSpeech/obama-10-utterance\n",
            "LibriSpeech: 100% 1/1 [00:13<00:00, 13.97s/speakers]\n",
            "The dataset consists of 11 utterances, 7807 mel frames, 1560000 audio timesteps (0.03 hours).\n",
            "Max input length (text chars): 206\n",
            "Max mel frames length: 833\n",
            "Max audio timesteps length: 166560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJXT68hyxbGA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5B8BOHQc6n-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db0226d-6238-42a9-c4a0-de5f74f93682"
      },
      "source": [
        "!python synthesizer_preprocess_embeds.py datasets_root/SV2TTS/synthesizer"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arguments:\n",
            "    synthesizer_root:      datasets_root/SV2TTS/synthesizer\n",
            "    encoder_model_fpath:   encoder/saved_models/pretrained.pt\n",
            "    n_processes:           4\n",
            "\n",
            "\rEmbedding:   0% 0/11 [00:00<?, ?utterances/s]Loaded encoder \"pretrained.pt\" trained to step 1564501\n",
            "Loaded encoder \"pretrained.pt\" trained to step 1564501\n",
            "Loaded encoder \"pretrained.pt\" trained to step 1564501\n",
            "Embedding:   9% 1/11 [00:20<03:25, 20.52s/utterances]Loaded encoder \"pretrained.pt\" trained to step 1564501\n",
            "Process ForkPoolWorker-5:\n",
            "Process ForkPoolWorker-2:\n",
            "Process ForkPoolWorker-6:\n",
            "Process ForkPoolWorker-1:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 720, in next\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"synthesizer_preprocess_embeds.py\", line 25, in <module>\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "    create_embeddings(**vars(args))    \n",
            "  File \"/content/gdrive/My Drive/Real-Time-Voice-Cloning/synthesizer/preprocess.py\", line 260, in create_embeddings\n",
            "    list(tqdm(job, \"Embedding\", len(fpaths), unit=\"utterances\"))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/std.py\", line 1104, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 724, in next\n",
            "    self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 295, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n",
            "Embedding:   9% 1/11 [30:33<5:05:31, 1833.17s/utterances]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjgS-NuSu3Pg",
        "outputId": "cad299cc-fcd1-49df-8eae-4da6f3e665d4"
      },
      "source": [
        "!python synthesizer_train.py pretrained-10 datasets_root/SV2TTS/synthesizer --checkpoint_interval 500"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arguments:\n",
            "    name:                   pretrained-10\n",
            "    synthesizer_root:       datasets_root/SV2TTS/synthesizer\n",
            "    models_dir:             synthesizer/saved_models/\n",
            "    mode:                   synthesis\n",
            "    GTA:                    True\n",
            "    restore:                True\n",
            "    summary_interval:       2500\n",
            "    embedding_interval:     10000\n",
            "    checkpoint_interval:    500\n",
            "    eval_interval:          100000\n",
            "    tacotron_train_steps:   2000000\n",
            "    tf_log_level:           1\n",
            "    slack_url:              None\n",
            "    hparams:                \n",
            "\n",
            "Checkpoint path: synthesizer/saved_models/logs-pretrained-10/taco_pretrained/tacotron_model.ckpt\n",
            "Loading training data from: datasets_root/SV2TTS/synthesizer/train.txt\n",
            "Using model: Tacotron\n",
            "Hyperparameters:\n",
            "  allow_clipping_in_normalization: True\n",
            "  attention_dim: 128\n",
            "  attention_filters: 32\n",
            "  attention_kernel: (31,)\n",
            "  cbhg_conv_channels: 128\n",
            "  cbhg_highway_units: 128\n",
            "  cbhg_highwaynet_layers: 4\n",
            "  cbhg_kernels: 8\n",
            "  cbhg_pool_size: 2\n",
            "  cbhg_projection: 256\n",
            "  cbhg_projection_kernel_size: 3\n",
            "  cbhg_rnn_units: 128\n",
            "  cleaners: english_cleaners\n",
            "  clip_for_wavenet: True\n",
            "  clip_mels_length: True\n",
            "  cross_entropy_pos_weight: 20\n",
            "  cumulative_weights: True\n",
            "  decoder_layers: 2\n",
            "  decoder_lstm_units: 1024\n",
            "  embedding_dim: 512\n",
            "  enc_conv_channels: 512\n",
            "  enc_conv_kernel_size: (5,)\n",
            "  enc_conv_num_layers: 3\n",
            "  encoder_lstm_units: 256\n",
            "  fmax: 7600\n",
            "  fmin: 55\n",
            "  frame_shift_ms: None\n",
            "  griffin_lim_iters: 60\n",
            "  hop_size: 200\n",
            "  mask_decoder: False\n",
            "  mask_encoder: True\n",
            "  max_abs_value: 4.0\n",
            "  max_iters: 2000\n",
            "  max_mel_frames: 900\n",
            "  min_level_db: -100\n",
            "  n_fft: 800\n",
            "  natural_eval: False\n",
            "  normalize_for_wavenet: True\n",
            "  num_mels: 80\n",
            "  outputs_per_step: 2\n",
            "  postnet_channels: 512\n",
            "  postnet_kernel_size: (5,)\n",
            "  postnet_num_layers: 5\n",
            "  power: 1.5\n",
            "  predict_linear: False\n",
            "  preemphasis: 0.97\n",
            "  preemphasize: True\n",
            "  prenet_layers: [256, 256]\n",
            "  ref_level_db: 20\n",
            "  rescale: True\n",
            "  rescaling_max: 0.9\n",
            "  sample_rate: 16000\n",
            "  signal_normalization: True\n",
            "  silence_min_duration_split: 0.05\n",
            "  silence_threshold: 2\n",
            "  smoothing: False\n",
            "  speaker_embedding_size: 256\n",
            "  split_on_cpu: True\n",
            "  stop_at_any: True\n",
            "  symmetric_mels: True\n",
            "  tacotron_adam_beta1: 0.9\n",
            "  tacotron_adam_beta2: 0.999\n",
            "  tacotron_adam_epsilon: 1e-06\n",
            "  tacotron_batch_size: 36\n",
            "  tacotron_clip_gradients: True\n",
            "  tacotron_data_random_state: 1234\n",
            "  tacotron_decay_learning_rate: True\n",
            "  tacotron_decay_rate: 0.5\n",
            "  tacotron_decay_steps: 50000\n",
            "  tacotron_dropout_rate: 0.5\n",
            "  tacotron_final_learning_rate: 1e-05\n",
            "  tacotron_gpu_start_idx: 0\n",
            "  tacotron_initial_learning_rate: 0.001\n",
            "  tacotron_num_gpus: 1\n",
            "  tacotron_random_seed: 5339\n",
            "  tacotron_reg_weight: 1e-07\n",
            "  tacotron_scale_regularization: False\n",
            "  tacotron_start_decay: 50000\n",
            "  tacotron_swap_with_cpu: False\n",
            "  tacotron_synthesis_batch_size: 128\n",
            "  tacotron_teacher_forcing_decay_alpha: 0.0\n",
            "  tacotron_teacher_forcing_decay_steps: 280000\n",
            "  tacotron_teacher_forcing_final_ratio: 0.0\n",
            "  tacotron_teacher_forcing_init_ratio: 1.0\n",
            "  tacotron_teacher_forcing_mode: constant\n",
            "  tacotron_teacher_forcing_ratio: 1.0\n",
            "  tacotron_teacher_forcing_start_decay: 10000\n",
            "  tacotron_test_batches: None\n",
            "  tacotron_test_size: 0.05\n",
            "  tacotron_zoneout_rate: 0.1\n",
            "  train_with_GTA: False\n",
            "  trim_fft_size: 512\n",
            "  trim_hop_size: 128\n",
            "  trim_silences: True\n",
            "  trim_top_db: 23\n",
            "  use_lws: False\n",
            "  utterance_min_duration: 1.6\n",
            "  win_size: 800\n",
            "Loaded metadata for 11 examples (0.03 hours)\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Real-Time-Voice-Cloning/synthesizer/models/modules.py:113: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Real-Time-Voice-Cloning/synthesizer/models/modules.py:420: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv1D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:218: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Real-Time-Voice-Cloning/synthesizer/models/modules.py:421: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Real-Time-Voice-Cloning/synthesizer/models/modules.py:424: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Real-Time-Voice-Cloning/synthesizer/models/modules.py:234: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Real-Time-Voice-Cloning/synthesizer/models/modules.py:148: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Real-Time-Voice-Cloning/synthesizer/models/modules.py:304: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Real-Time-Voice-Cloning/synthesizer/models/modules.py:268: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "initialisation done /gpu:0\n",
            "Initialized Tacotron model. Dimensions (? = dynamic shape): \n",
            "  Train mode:               True\n",
            "  Eval mode:                False\n",
            "  GTA mode:                 False\n",
            "  Synthesis mode:           False\n",
            "  Input:                    (?, ?)\n",
            "  device:                   0\n",
            "  embedding:                (?, ?, 512)\n",
            "  enc conv out:             (?, ?, 512)\n",
            "  encoder out (cond):       (?, ?, 768)\n",
            "  decoder out:              (?, ?, 80)\n",
            "  residual out:             (?, ?, 512)\n",
            "  projected residual out:   (?, ?, 80)\n",
            "  mel out:                  (?, ?, 80)\n",
            "  <stop_token> out:         (?, ?)\n",
            "  Tacotron Parameters       28.439 Million.\n",
            "initialisation done /gpu:0\n",
            "Initialized Tacotron model. Dimensions (? = dynamic shape): \n",
            "  Train mode:               False\n",
            "  Eval mode:                True\n",
            "  GTA mode:                 False\n",
            "  Synthesis mode:           False\n",
            "  Input:                    (?, ?)\n",
            "  device:                   0\n",
            "  embedding:                (?, ?, 512)\n",
            "  enc conv out:             (?, ?, 512)\n",
            "  encoder out (cond):       (?, ?, 768)\n",
            "  decoder out:              (?, ?, 80)\n",
            "  residual out:             (?, ?, 512)\n",
            "  projected residual out:   (?, ?, 80)\n",
            "  mel out:                  (?, ?, 80)\n",
            "  <stop_token> out:         (?, ?)\n",
            "  Tacotron Parameters       28.439 Million.\n",
            "Tacotron training set to a maximum of 2000000 steps\n",
            "Loading checkpoint synthesizer/saved_models/logs-pretrained-10/taco_pretrained/tacotron_model.ckpt-280500\n",
            "\n",
            "Generated 0 test batches of size 36 in 0.000 sec\n",
            "\n",
            "Generated 64 train batches of size 36 in 128.959 sec\n",
            "Step  280501 [139.358 sec/step, loss=0.28611, avg_loss=0.28611]\n",
            "Step  280502 [71.564 sec/step, loss=0.28313, avg_loss=0.28462]\n",
            "Step  280503 [49.658 sec/step, loss=0.29743, avg_loss=0.28889]\n",
            "Step  280504 [38.344 sec/step, loss=0.29763, avg_loss=0.29108]\n",
            "Step  280505 [31.575 sec/step, loss=0.29684, avg_loss=0.29223]\n",
            "Step  280506 [27.069 sec/step, loss=0.29795, avg_loss=0.29318]\n",
            "Step  280507 [24.087 sec/step, loss=0.30848, avg_loss=0.29537]\n",
            "Step  280508 [21.673 sec/step, loss=0.28706, avg_loss=0.29433]\n",
            "Step  280509 [19.773 sec/step, loss=0.29707, avg_loss=0.29463]\n",
            "Step  280510 [18.295 sec/step, loss=0.30864, avg_loss=0.29603]\n",
            "Step  280511 [17.042 sec/step, loss=0.30088, avg_loss=0.29648]\n",
            "Step  280512 [15.938 sec/step, loss=0.28259, avg_loss=0.29532]\n",
            "Step  280513 [15.104 sec/step, loss=0.29602, avg_loss=0.29537]\n",
            "Step  280514 [14.344 sec/step, loss=0.29616, avg_loss=0.29543]\n",
            "Step  280515 [13.693 sec/step, loss=0.30663, avg_loss=0.29618]\n",
            "Step  280516 [13.073 sec/step, loss=0.28039, avg_loss=0.29519]\n",
            "Step  280517 [12.550 sec/step, loss=0.29559, avg_loss=0.29521]\n",
            "Step  280518 [12.136 sec/step, loss=0.30329, avg_loss=0.29566]\n",
            "Step  280519 [11.756 sec/step, loss=0.29308, avg_loss=0.29553]\n",
            "Step  280520 [11.315 sec/step, loss=0.29668, avg_loss=0.29558]\n",
            "Step  280521 [11.021 sec/step, loss=0.30619, avg_loss=0.29609]\n",
            "Step  280522 [10.724 sec/step, loss=0.29498, avg_loss=0.29604]\n",
            "Step  280523 [10.403 sec/step, loss=0.28124, avg_loss=0.29539]\n",
            "Step  280524 [10.150 sec/step, loss=0.29426, avg_loss=0.29535]\n",
            "Step  280525 [9.896 sec/step, loss=0.29688, avg_loss=0.29541]\n",
            "Step  280526 [9.660 sec/step, loss=0.28666, avg_loss=0.29507]\n",
            "Step  280527 [9.457 sec/step, loss=0.29332, avg_loss=0.29501]\n",
            "Step  280528 [9.259 sec/step, loss=0.28586, avg_loss=0.29468]\n",
            "Step  280529 [9.087 sec/step, loss=0.30283, avg_loss=0.29496]\n",
            "Step  280530 [8.930 sec/step, loss=0.30240, avg_loss=0.29521]\n",
            "Step  280531 [8.777 sec/step, loss=0.29382, avg_loss=0.29516]\n",
            "Step  280532 [8.635 sec/step, loss=0.29535, avg_loss=0.29517]\n",
            "Step  280533 [8.500 sec/step, loss=0.29578, avg_loss=0.29519]\n",
            "Step  280534 [8.378 sec/step, loss=0.29183, avg_loss=0.29509]\n",
            "Step  280535 [8.266 sec/step, loss=0.30208, avg_loss=0.29529]\n",
            "Step  280536 [8.157 sec/step, loss=0.29523, avg_loss=0.29529]\n",
            "Step  280537 [8.045 sec/step, loss=0.29512, avg_loss=0.29528]\n",
            "Step  280538 [7.938 sec/step, loss=0.29337, avg_loss=0.29523]\n",
            "Step  280539 [7.839 sec/step, loss=0.30051, avg_loss=0.29537]\n",
            "Step  280540 [7.731 sec/step, loss=0.28205, avg_loss=0.29504]\n",
            "Step  280541 [7.650 sec/step, loss=0.30059, avg_loss=0.29517]\n",
            "Step  280542 [7.553 sec/step, loss=0.29205, avg_loss=0.29510]\n",
            "Step  280543 [7.470 sec/step, loss=0.28219, avg_loss=0.29480]\n",
            "Step  280544 [7.405 sec/step, loss=0.29535, avg_loss=0.29481]\n",
            "Step  280545 [7.347 sec/step, loss=0.29218, avg_loss=0.29475]\n",
            "Step  280546 [7.288 sec/step, loss=0.29499, avg_loss=0.29476]\n",
            "Step  280547 [7.235 sec/step, loss=0.29816, avg_loss=0.29483]\n",
            "Step  280548 [7.177 sec/step, loss=0.29955, avg_loss=0.29493]\n",
            "Step  280549 [7.108 sec/step, loss=0.29387, avg_loss=0.29491]\n",
            "Step  280550 [7.035 sec/step, loss=0.28532, avg_loss=0.29471]\n",
            "Step  280551 [6.985 sec/step, loss=0.29026, avg_loss=0.29463]\n",
            "Step  280552 [6.932 sec/step, loss=0.30027, avg_loss=0.29473]\n",
            "Step  280553 [6.875 sec/step, loss=0.29678, avg_loss=0.29477]\n",
            "Step  280554 [6.827 sec/step, loss=0.29172, avg_loss=0.29472]\n",
            "Step  280555 [6.779 sec/step, loss=0.29999, avg_loss=0.29481]\n",
            "Step  280556 [6.752 sec/step, loss=0.29479, avg_loss=0.29481]\n",
            "Step  280557 [6.702 sec/step, loss=0.29612, avg_loss=0.29484]\n",
            "Step  280558 [6.678 sec/step, loss=0.30396, avg_loss=0.29499]\n",
            "Step  280559 [6.650 sec/step, loss=0.30709, avg_loss=0.29520]\n",
            "Step  280560 [6.605 sec/step, loss=0.28507, avg_loss=0.29503]\n",
            "Step  280561 [6.569 sec/step, loss=0.29118, avg_loss=0.29497]\n",
            "Step  280562 [6.535 sec/step, loss=0.28442, avg_loss=0.29480]\n",
            "Step  280563 [6.495 sec/step, loss=0.29556, avg_loss=0.29481]\n",
            "Step  280564 [6.461 sec/step, loss=0.29320, avg_loss=0.29478]\n",
            "\n",
            "Generated 64 train batches of size 36 in 102.491 sec\n",
            "Step  280565 [7.394 sec/step, loss=0.29476, avg_loss=0.29478]\n",
            "Step  280566 [7.358 sec/step, loss=0.30734, avg_loss=0.29497]\n",
            "Step  280567 [7.311 sec/step, loss=0.29266, avg_loss=0.29494]\n",
            "Step  280568 [7.272 sec/step, loss=0.30031, avg_loss=0.29502]\n",
            "Step  280569 [7.233 sec/step, loss=0.29431, avg_loss=0.29501]\n",
            "Step  280570 [7.183 sec/step, loss=0.27927, avg_loss=0.29478]\n",
            "Step  280571 [7.143 sec/step, loss=0.28979, avg_loss=0.29471]\n",
            "Step  280572 [7.098 sec/step, loss=0.28363, avg_loss=0.29456]\n",
            "Step  280573 [7.047 sec/step, loss=0.29156, avg_loss=0.29452]\n",
            "Step  280574 [7.009 sec/step, loss=0.29420, avg_loss=0.29451]\n",
            "Step  280575 [6.973 sec/step, loss=0.29763, avg_loss=0.29455]\n",
            "Step  280576 [6.935 sec/step, loss=0.28560, avg_loss=0.29444]\n",
            "Step  280577 [6.888 sec/step, loss=0.29399, avg_loss=0.29443]\n",
            "Step  280578 [6.849 sec/step, loss=0.29118, avg_loss=0.29439]\n",
            "Step  280579 [6.809 sec/step, loss=0.29099, avg_loss=0.29435]\n",
            "Step  280580 [6.767 sec/step, loss=0.28196, avg_loss=0.29419]\n",
            "Step  280581 [6.732 sec/step, loss=0.29123, avg_loss=0.29415]\n",
            "Step  280582 [6.700 sec/step, loss=0.29002, avg_loss=0.29410]\n",
            "Step  280583 [6.667 sec/step, loss=0.29277, avg_loss=0.29409]\n",
            "Step  280584 [6.639 sec/step, loss=0.29457, avg_loss=0.29409]\n",
            "Step  280585 [6.601 sec/step, loss=0.28308, avg_loss=0.29396]\n",
            "Step  280586 [6.572 sec/step, loss=0.29903, avg_loss=0.29402]\n",
            "Step  280587 [6.541 sec/step, loss=0.28079, avg_loss=0.29387]\n",
            "Step  280588 [6.518 sec/step, loss=0.28923, avg_loss=0.29382]\n",
            "Step  280589 [6.488 sec/step, loss=0.27960, avg_loss=0.29366]\n",
            "Step  280590 [6.464 sec/step, loss=0.29259, avg_loss=0.29365]\n",
            "Step  280591 [6.432 sec/step, loss=0.28034, avg_loss=0.29350]\n",
            "Step  280592 [6.410 sec/step, loss=0.29737, avg_loss=0.29354]\n",
            "Step  280593 [6.385 sec/step, loss=0.29617, avg_loss=0.29357]\n",
            "Step  280594 [6.363 sec/step, loss=0.29883, avg_loss=0.29363]\n",
            "Step  280595 [6.351 sec/step, loss=0.30700, avg_loss=0.29377]\n",
            "Step  280596 [6.329 sec/step, loss=0.28894, avg_loss=0.29372]\n",
            "Step  280597 [6.306 sec/step, loss=0.29218, avg_loss=0.29370]\n",
            "Step  280598 [6.283 sec/step, loss=0.29917, avg_loss=0.29376]\n",
            "Step  280599 [6.257 sec/step, loss=0.29260, avg_loss=0.29375]\n",
            "Step  280600 [6.245 sec/step, loss=0.30665, avg_loss=0.29387]\n",
            "Step  280601 [4.890 sec/step, loss=0.29804, avg_loss=0.29399]\n",
            "Step  280602 [4.894 sec/step, loss=0.29248, avg_loss=0.29409]\n",
            "Step  280603 [4.882 sec/step, loss=0.29401, avg_loss=0.29405]\n",
            "Step  280604 [4.881 sec/step, loss=0.29394, avg_loss=0.29402]\n",
            "Step  280605 [4.882 sec/step, loss=0.29464, avg_loss=0.29399]\n",
            "Step  280606 [4.882 sec/step, loss=0.29501, avg_loss=0.29397]\n",
            "Step  280607 [4.862 sec/step, loss=0.28229, avg_loss=0.29370]\n",
            "Step  280608 [4.848 sec/step, loss=0.29225, avg_loss=0.29376]\n",
            "Step  280609 [4.844 sec/step, loss=0.29839, avg_loss=0.29377]\n",
            "Step  280610 [4.836 sec/step, loss=0.29319, avg_loss=0.29361]\n",
            "Step  280611 [4.836 sec/step, loss=0.29726, avg_loss=0.29358]\n",
            "Step  280612 [4.841 sec/step, loss=0.29805, avg_loss=0.29373]\n",
            "Step  280613 [4.833 sec/step, loss=0.28866, avg_loss=0.29366]\n",
            "Step  280614 [4.830 sec/step, loss=0.28778, avg_loss=0.29357]\n",
            "Step  280615 [4.822 sec/step, loss=0.28308, avg_loss=0.29334]\n",
            "Step  280616 [4.832 sec/step, loss=0.30595, avg_loss=0.29359]\n",
            "Step  280617 [4.833 sec/step, loss=0.29111, avg_loss=0.29355]\n",
            "Step  280618 [4.826 sec/step, loss=0.29675, avg_loss=0.29348]\n",
            "Step  280619 [4.824 sec/step, loss=0.29329, avg_loss=0.29349]\n",
            "Step  280620 [4.834 sec/step, loss=0.29137, avg_loss=0.29343]\n",
            "Step  280621 [4.829 sec/step, loss=0.27992, avg_loss=0.29317]\n",
            "Step  280622 [4.826 sec/step, loss=0.29010, avg_loss=0.29312]\n",
            "Step  280623 [4.844 sec/step, loss=0.30523, avg_loss=0.29336]\n",
            "Step  280624 [4.845 sec/step, loss=0.28883, avg_loss=0.29331]\n",
            "Step  280625 [4.858 sec/step, loss=0.29212, avg_loss=0.29326]\n",
            "Step  280626 [4.857 sec/step, loss=0.28357, avg_loss=0.29323]\n",
            "Step  280627 [4.868 sec/step, loss=0.30372, avg_loss=0.29333]\n",
            "Step  280628 [4.870 sec/step, loss=0.28331, avg_loss=0.29331]\n",
            "\n",
            "Generated 64 train batches of size 36 in 80.037 sec\n",
            "Step  280629 [5.267 sec/step, loss=0.28946, avg_loss=0.29317]\n",
            "Step  280630 [5.261 sec/step, loss=0.28259, avg_loss=0.29298]\n",
            "Step  280631 [5.262 sec/step, loss=0.28579, avg_loss=0.29290]\n",
            "Step  280632 [5.258 sec/step, loss=0.29360, avg_loss=0.29288]\n",
            "Step  280633 [5.265 sec/step, loss=0.29202, avg_loss=0.29284]\n",
            "Step  280634 [5.261 sec/step, loss=0.29051, avg_loss=0.29283]\n",
            "Step  280635 [5.259 sec/step, loss=0.29700, avg_loss=0.29278]\n",
            "Step  280636 [5.251 sec/step, loss=0.28113, avg_loss=0.29264]\n",
            "Step  280637 [5.262 sec/step, loss=0.29246, avg_loss=0.29261]\n",
            "Step  280638 [5.269 sec/step, loss=0.30351, avg_loss=0.29271]\n",
            "Step  280639 [5.283 sec/step, loss=0.30388, avg_loss=0.29274]\n",
            "Step  280640 [5.284 sec/step, loss=0.28945, avg_loss=0.29282]\n",
            "Step  280641 [5.284 sec/step, loss=0.29211, avg_loss=0.29273]\n",
            "Step  280642 [5.283 sec/step, loss=0.28033, avg_loss=0.29262]\n",
            "Step  280643 [5.273 sec/step, loss=0.27928, avg_loss=0.29259]\n",
            "Step  280644 [5.264 sec/step, loss=0.28844, avg_loss=0.29252]\n",
            "Step  280645 [5.263 sec/step, loss=0.30138, avg_loss=0.29261]\n",
            "Step  280646 [5.255 sec/step, loss=0.28030, avg_loss=0.29246]\n",
            "Step  280647 [5.244 sec/step, loss=0.28784, avg_loss=0.29236]\n",
            "Step  280648 [5.241 sec/step, loss=0.28840, avg_loss=0.29225]\n",
            "Step  280649 [5.247 sec/step, loss=0.28702, avg_loss=0.29218]\n",
            "Step  280650 [5.253 sec/step, loss=0.29149, avg_loss=0.29224]\n",
            "Step  280651 [5.251 sec/step, loss=0.29194, avg_loss=0.29226]\n",
            "Step  280652 [5.253 sec/step, loss=0.30211, avg_loss=0.29228]\n",
            "Step  280653 [5.266 sec/step, loss=0.30185, avg_loss=0.29233]\n",
            "Step  280654 [5.267 sec/step, loss=0.29720, avg_loss=0.29238]\n",
            "Step  280655 [5.269 sec/step, loss=0.28809, avg_loss=0.29226]\n",
            "Step  280656 [5.265 sec/step, loss=0.29194, avg_loss=0.29223]\n",
            "Step  280657 [5.263 sec/step, loss=0.27942, avg_loss=0.29207]\n",
            "Step  280658 [5.247 sec/step, loss=0.29157, avg_loss=0.29194]\n",
            "Step  280659 [5.238 sec/step, loss=0.29141, avg_loss=0.29179]\n",
            "Step  280660 [5.237 sec/step, loss=0.29076, avg_loss=0.29184]\n",
            "Step  280661 [5.232 sec/step, loss=0.29664, avg_loss=0.29190]\n",
            "Step  280662 [5.222 sec/step, loss=0.27825, avg_loss=0.29184]\n",
            "Step  280663 [5.222 sec/step, loss=0.27583, avg_loss=0.29164]\n",
            "Step  280664 [5.217 sec/step, loss=0.28678, avg_loss=0.29157]\n",
            "Step  280665 [4.586 sec/step, loss=0.27971, avg_loss=0.29142]\n",
            "Step  280666 [4.573 sec/step, loss=0.29112, avg_loss=0.29126]\n",
            "Step  280667 [4.569 sec/step, loss=0.28987, avg_loss=0.29123]\n",
            "Step  280668 [4.567 sec/step, loss=0.29575, avg_loss=0.29119]\n",
            "Step  280669 [4.568 sec/step, loss=0.29110, avg_loss=0.29116]\n",
            "Step  280670 [4.571 sec/step, loss=0.29634, avg_loss=0.29133]\n",
            "Step  280671 [4.571 sec/step, loss=0.29117, avg_loss=0.29134]\n",
            "Step  280672 [4.569 sec/step, loss=0.27772, avg_loss=0.29128]\n",
            "Step  280673 [4.584 sec/step, loss=0.29080, avg_loss=0.29127]\n",
            "Step  280674 [4.590 sec/step, loss=0.30097, avg_loss=0.29134]\n",
            "Step  280675 [4.593 sec/step, loss=0.29560, avg_loss=0.29132]\n",
            "Step  280676 [4.585 sec/step, loss=0.27604, avg_loss=0.29123]\n",
            "Step  280677 [4.592 sec/step, loss=0.29068, avg_loss=0.29119]\n",
            "Step  280678 [4.597 sec/step, loss=0.29663, avg_loss=0.29125]\n",
            "Step  280679 [4.600 sec/step, loss=0.28819, avg_loss=0.29122]\n",
            "Step  280680 [4.608 sec/step, loss=0.28661, avg_loss=0.29127]\n",
            "Step  280681 [4.611 sec/step, loss=0.29599, avg_loss=0.29131]\n",
            "Step  280682 [4.612 sec/step, loss=0.28586, avg_loss=0.29127]\n",
            "Step  280683 [4.620 sec/step, loss=0.29581, avg_loss=0.29130]\n",
            "Step  280684 [4.617 sec/step, loss=0.28116, avg_loss=0.29117]\n",
            "Step  280685 [4.639 sec/step, loss=0.29326, avg_loss=0.29127]\n",
            "Step  280686 [4.643 sec/step, loss=0.29473, avg_loss=0.29123]\n",
            "Step  280687 [4.645 sec/step, loss=0.28963, avg_loss=0.29132]\n",
            "Step  280688 [4.640 sec/step, loss=0.28966, avg_loss=0.29132]\n",
            "Step  280689 [4.652 sec/step, loss=0.29084, avg_loss=0.29143]\n",
            "Step  280690 [4.647 sec/step, loss=0.27947, avg_loss=0.29130]\n",
            "Step  280691 [4.657 sec/step, loss=0.29057, avg_loss=0.29140]\n",
            "Step  280692 [4.653 sec/step, loss=0.28956, avg_loss=0.29132]\n",
            "\n",
            "Generated 64 train batches of size 36 in 91.379 sec\n",
            "Step  280693 [5.167 sec/step, loss=0.27575, avg_loss=0.29112]\n",
            "Step  280694 [5.166 sec/step, loss=0.29466, avg_loss=0.29108]\n",
            "Step  280695 [5.162 sec/step, loss=0.30064, avg_loss=0.29102]\n",
            "Step  280696 [5.165 sec/step, loss=0.29005, avg_loss=0.29103]\n",
            "Step  280697 [5.167 sec/step, loss=0.28653, avg_loss=0.29097]\n",
            "Step  280698 [5.174 sec/step, loss=0.29328, avg_loss=0.29091]\n",
            "Step  280699 [5.183 sec/step, loss=0.27662, avg_loss=0.29075]\n",
            "Step  280700 [5.184 sec/step, loss=0.30154, avg_loss=0.29070]\n",
            "Step  280701 [5.197 sec/step, loss=0.29032, avg_loss=0.29062]\n",
            "Step  280702 [5.192 sec/step, loss=0.27595, avg_loss=0.29046]\n",
            "Step  280703 [5.183 sec/step, loss=0.28776, avg_loss=0.29040]\n",
            "Step  280704 [5.176 sec/step, loss=0.27955, avg_loss=0.29025]\n",
            "Step  280705 [5.171 sec/step, loss=0.28800, avg_loss=0.29018]\n",
            "Step  280706 [5.170 sec/step, loss=0.28988, avg_loss=0.29013]\n",
            "Step  280707 [5.177 sec/step, loss=0.28971, avg_loss=0.29021]\n",
            "Step  280708 [5.180 sec/step, loss=0.28786, avg_loss=0.29016]\n",
            "Step  280709 [5.186 sec/step, loss=0.28940, avg_loss=0.29007]\n",
            "Step  280710 [5.178 sec/step, loss=0.27933, avg_loss=0.28994]\n",
            "Step  280711 [5.175 sec/step, loss=0.29072, avg_loss=0.28987]\n",
            "Step  280712 [5.173 sec/step, loss=0.28940, avg_loss=0.28978]\n",
            "Step  280713 [5.172 sec/step, loss=0.28694, avg_loss=0.28977]\n",
            "Step  280714 [5.168 sec/step, loss=0.28823, avg_loss=0.28977]\n",
            "Step  280715 [5.176 sec/step, loss=0.28962, avg_loss=0.28984]\n",
            "Step  280716 [5.169 sec/step, loss=0.28527, avg_loss=0.28963]\n",
            "Step  280717 [5.165 sec/step, loss=0.28461, avg_loss=0.28956]\n",
            "Step  280718 [5.171 sec/step, loss=0.30015, avg_loss=0.28960]\n",
            "Step  280719 [5.175 sec/step, loss=0.29180, avg_loss=0.28958]\n",
            "Step  280720 [5.171 sec/step, loss=0.28661, avg_loss=0.28954]\n",
            "Step  280721 [5.162 sec/step, loss=0.27787, avg_loss=0.28952]\n",
            "Step  280722 [5.157 sec/step, loss=0.28816, avg_loss=0.28950]\n",
            "Step  280723 [5.145 sec/step, loss=0.27619, avg_loss=0.28921]\n",
            "Step  280724 [5.147 sec/step, loss=0.28976, avg_loss=0.28921]\n",
            "Step  280725 [5.143 sec/step, loss=0.28880, avg_loss=0.28918]\n",
            "Step  280726 [5.146 sec/step, loss=0.28756, avg_loss=0.28922]\n",
            "Step  280727 [5.129 sec/step, loss=0.28699, avg_loss=0.28905]\n",
            "Step  280728 [5.126 sec/step, loss=0.27576, avg_loss=0.28898]\n",
            "Step  280729 [4.725 sec/step, loss=0.28610, avg_loss=0.28895]\n",
            "Step  280730 [4.730 sec/step, loss=0.28468, avg_loss=0.28897]\n",
            "Step  280731 [4.729 sec/step, loss=0.28511, avg_loss=0.28896]\n",
            "Step  280732 [4.735 sec/step, loss=0.29402, avg_loss=0.28896]\n",
            "Step  280733 [4.727 sec/step, loss=0.29461, avg_loss=0.28899]\n",
            "Step  280734 [4.726 sec/step, loss=0.28658, avg_loss=0.28895]\n",
            "Step  280735 [4.721 sec/step, loss=0.27209, avg_loss=0.28870]\n",
            "Step  280736 [4.733 sec/step, loss=0.28965, avg_loss=0.28879]\n",
            "Step  280737 [4.722 sec/step, loss=0.27813, avg_loss=0.28864]\n",
            "Step  280738 [4.712 sec/step, loss=0.27448, avg_loss=0.28835]\n",
            "Step  280739 [4.697 sec/step, loss=0.28793, avg_loss=0.28819]\n",
            "Step  280740 [4.708 sec/step, loss=0.30153, avg_loss=0.28831]\n",
            "Step  280741 [4.703 sec/step, loss=0.29439, avg_loss=0.28834]\n",
            "Step  280742 [4.711 sec/step, loss=0.28847, avg_loss=0.28842]\n",
            "Step  280743 [4.722 sec/step, loss=0.28582, avg_loss=0.28848]\n",
            "Step  280744 [4.729 sec/step, loss=0.29821, avg_loss=0.28858]\n",
            "Step  280745 [4.729 sec/step, loss=0.29444, avg_loss=0.28851]\n",
            "Step  280746 [4.732 sec/step, loss=0.27807, avg_loss=0.28849]\n",
            "Step  280747 [4.740 sec/step, loss=0.30110, avg_loss=0.28862]\n",
            "Step  280748 [4.748 sec/step, loss=0.29350, avg_loss=0.28867]\n",
            "Step  280749 [4.743 sec/step, loss=0.28825, avg_loss=0.28869]\n",
            "Step  280750 [4.752 sec/step, loss=0.29288, avg_loss=0.28870]\n",
            "Step  280751 [4.747 sec/step, loss=0.27736, avg_loss=0.28855]\n",
            "Step  280752 [4.745 sec/step, loss=0.28712, avg_loss=0.28840]\n",
            "Step  280753 [4.739 sec/step, loss=0.29298, avg_loss=0.28831]\n",
            "Step  280754 [4.744 sec/step, loss=0.29420, avg_loss=0.28828]\n",
            "Step  280755 [4.754 sec/step, loss=0.29184, avg_loss=0.28832]\n",
            "Step  280756 [4.763 sec/step, loss=0.28862, avg_loss=0.28829]\n",
            "\n",
            "Generated 64 train batches of size 36 in 86.554 sec\n",
            "Step  280757 [5.213 sec/step, loss=0.27654, avg_loss=0.28826]\n",
            "Step  280758 [5.209 sec/step, loss=0.27472, avg_loss=0.28809]\n",
            "Step  280759 [5.208 sec/step, loss=0.29144, avg_loss=0.28809]\n",
            "Step  280760 [5.221 sec/step, loss=0.29976, avg_loss=0.28818]\n",
            "Step  280761 [5.224 sec/step, loss=0.28589, avg_loss=0.28807]\n",
            "Step  280762 [5.241 sec/step, loss=0.28879, avg_loss=0.28818]\n",
            "Step  280763 [5.242 sec/step, loss=0.28623, avg_loss=0.28828]\n",
            "Step  280764 [5.245 sec/step, loss=0.27632, avg_loss=0.28818]\n",
            "Step  280765 [5.255 sec/step, loss=0.28776, avg_loss=0.28826]\n",
            "Step  280766 [5.266 sec/step, loss=0.28796, avg_loss=0.28823]\n",
            "Step  280767 [5.259 sec/step, loss=0.28397, avg_loss=0.28817]\n",
            "Step  280768 [5.260 sec/step, loss=0.28868, avg_loss=0.28810]\n",
            "Step  280769 [5.258 sec/step, loss=0.29244, avg_loss=0.28811]\n",
            "Step  280770 [5.258 sec/step, loss=0.28544, avg_loss=0.28800]\n",
            "Step  280771 [5.260 sec/step, loss=0.29868, avg_loss=0.28808]\n",
            "Step  280772 [5.263 sec/step, loss=0.28753, avg_loss=0.28818]\n",
            "Step  280773 [5.261 sec/step, loss=0.28969, avg_loss=0.28817]\n",
            "Step  280774 [5.255 sec/step, loss=0.27741, avg_loss=0.28793]\n",
            "Step  280775 [5.250 sec/step, loss=0.28719, avg_loss=0.28785]\n",
            "Step  280776 [5.267 sec/step, loss=0.29121, avg_loss=0.28800]\n",
            "Step  280777 [5.267 sec/step, loss=0.28529, avg_loss=0.28794]\n",
            "Step  280778 [5.272 sec/step, loss=0.28515, avg_loss=0.28783]\n",
            "Step  280779 [5.275 sec/step, loss=0.28607, avg_loss=0.28781]\n",
            "Step  280780 [5.274 sec/step, loss=0.28585, avg_loss=0.28780]\n",
            "Step  280781 [5.275 sec/step, loss=0.28491, avg_loss=0.28769]\n",
            "Step  280782 [5.270 sec/step, loss=0.27619, avg_loss=0.28759]\n",
            "Step  280783 [5.263 sec/step, loss=0.27839, avg_loss=0.28742]\n",
            "Step  280784 [5.259 sec/step, loss=0.27105, avg_loss=0.28732]\n",
            "Step  280785 [5.254 sec/step, loss=0.28479, avg_loss=0.28723]\n",
            "Step  280786 [5.260 sec/step, loss=0.29727, avg_loss=0.28726]\n",
            "Step  280787 [5.267 sec/step, loss=0.29850, avg_loss=0.28735]\n",
            "Step  280788 [5.268 sec/step, loss=0.28977, avg_loss=0.28735]\n",
            "Step  280789 [5.268 sec/step, loss=0.28511, avg_loss=0.28729]\n",
            "Step  280790 [5.281 sec/step, loss=0.28666, avg_loss=0.28736]\n",
            "Step  280791 [5.278 sec/step, loss=0.28794, avg_loss=0.28734]\n",
            "Step  280792 [5.295 sec/step, loss=0.29019, avg_loss=0.28734]\n",
            "Step  280793 [4.784 sec/step, loss=0.29368, avg_loss=0.28752]\n",
            "Step  280794 [4.797 sec/step, loss=0.29870, avg_loss=0.28756]\n",
            "Step  280795 [4.797 sec/step, loss=0.28400, avg_loss=0.28740]\n",
            "Step  280796 [4.795 sec/step, loss=0.29189, avg_loss=0.28741]\n",
            "Step  280797 [4.793 sec/step, loss=0.29076, avg_loss=0.28746]\n",
            "Step  280798 [4.785 sec/step, loss=0.28655, avg_loss=0.28739]\n",
            "Step  280799 [4.778 sec/step, loss=0.27379, avg_loss=0.28736]\n",
            "Step  280800 [4.779 sec/step, loss=0.29816, avg_loss=0.28733]\n",
            "Step  280801 [4.767 sec/step, loss=0.27503, avg_loss=0.28717]\n",
            "Step  280802 [4.773 sec/step, loss=0.29393, avg_loss=0.28735]\n",
            "Step  280803 [4.783 sec/step, loss=0.28938, avg_loss=0.28737]\n",
            "Step  280804 [4.790 sec/step, loss=0.27679, avg_loss=0.28734]\n",
            "Step  280805 [4.790 sec/step, loss=0.28808, avg_loss=0.28734]\n",
            "Step  280806 [4.793 sec/step, loss=0.29190, avg_loss=0.28736]\n",
            "Step  280807 [4.785 sec/step, loss=0.28424, avg_loss=0.28731]\n",
            "Step  280808 [4.787 sec/step, loss=0.27476, avg_loss=0.28718]\n",
            "Step  280809 [4.781 sec/step, loss=0.28415, avg_loss=0.28712]\n",
            "Step  280810 [4.790 sec/step, loss=0.28687, avg_loss=0.28720]\n",
            "Step  280811 [4.790 sec/step, loss=0.28624, avg_loss=0.28716]\n",
            "Step  280812 [4.787 sec/step, loss=0.28551, avg_loss=0.28712]\n",
            "Step  280813 [4.789 sec/step, loss=0.28385, avg_loss=0.28709]\n",
            "Step  280814 [4.796 sec/step, loss=0.28496, avg_loss=0.28705]\n",
            "Step  280815 [4.797 sec/step, loss=0.29042, avg_loss=0.28706]\n",
            "Step  280816 [4.808 sec/step, loss=0.29761, avg_loss=0.28718]\n",
            "Step  280817 [4.814 sec/step, loss=0.28302, avg_loss=0.28717]\n",
            "Step  280818 [4.812 sec/step, loss=0.28289, avg_loss=0.28700]\n",
            "Step  280819 [4.801 sec/step, loss=0.27342, avg_loss=0.28681]\n",
            "Step  280820 [4.811 sec/step, loss=0.29008, avg_loss=0.28685]\n",
            "\n",
            "Generated 64 train batches of size 36 in 92.346 sec\n",
            "Step  280821 [5.342 sec/step, loss=0.28997, avg_loss=0.28697]\n",
            "Step  280822 [5.355 sec/step, loss=0.29768, avg_loss=0.28706]\n",
            "Step  280823 [5.356 sec/step, loss=0.28314, avg_loss=0.28713]\n",
            "Step  280824 [5.350 sec/step, loss=0.28304, avg_loss=0.28707]\n",
            "Step  280825 [5.348 sec/step, loss=0.28156, avg_loss=0.28699]\n",
            "Step  280826 [5.351 sec/step, loss=0.28940, avg_loss=0.28701]\n",
            "Step  280827 [5.358 sec/step, loss=0.28238, avg_loss=0.28697]\n",
            "Step  280828 [5.367 sec/step, loss=0.28054, avg_loss=0.28701]\n",
            "Step  280829 [5.380 sec/step, loss=0.28626, avg_loss=0.28701]\n",
            "Step  280830 [5.385 sec/step, loss=0.28672, avg_loss=0.28704]\n",
            "Step  280831 [5.389 sec/step, loss=0.28079, avg_loss=0.28699]\n",
            "Step  280832 [5.387 sec/step, loss=0.28836, avg_loss=0.28694]\n",
            "Step  280833 [5.390 sec/step, loss=0.28943, avg_loss=0.28688]\n",
            "Step  280834 [5.393 sec/step, loss=0.27338, avg_loss=0.28675]\n",
            "Step  280835 [5.397 sec/step, loss=0.26977, avg_loss=0.28673]\n",
            "Step  280836 [5.395 sec/step, loss=0.29014, avg_loss=0.28673]\n",
            "Step  280837 [5.394 sec/step, loss=0.27487, avg_loss=0.28670]\n",
            "Step  280838 [5.395 sec/step, loss=0.28413, avg_loss=0.28680]\n",
            "Step  280839 [5.404 sec/step, loss=0.29655, avg_loss=0.28688]\n",
            "Step  280840 [5.399 sec/step, loss=0.28868, avg_loss=0.28675]\n",
            "Step  280841 [5.397 sec/step, loss=0.28414, avg_loss=0.28665]\n",
            "Step  280842 [5.392 sec/step, loss=0.27478, avg_loss=0.28652]\n",
            "Step  280843 [5.387 sec/step, loss=0.27170, avg_loss=0.28637]\n",
            "Step  280844 [5.399 sec/step, loss=0.28593, avg_loss=0.28625]\n",
            "Step  280845 [5.396 sec/step, loss=0.28505, avg_loss=0.28616]\n",
            "Step  280846 [5.397 sec/step, loss=0.27365, avg_loss=0.28611]\n",
            "Step  280847 [5.393 sec/step, loss=0.28407, avg_loss=0.28594]\n",
            "Step  280848 [5.385 sec/step, loss=0.27492, avg_loss=0.28576]\n",
            "Step  280849 [5.389 sec/step, loss=0.27799, avg_loss=0.28565]\n",
            "Step  280850 [5.372 sec/step, loss=0.28486, avg_loss=0.28557]\n",
            "Step  280851 [5.378 sec/step, loss=0.27231, avg_loss=0.28552]\n",
            "Step  280852 [5.388 sec/step, loss=0.29663, avg_loss=0.28562]\n",
            "Step  280853 [5.386 sec/step, loss=0.28281, avg_loss=0.28552]\n",
            "Step  280854 [5.383 sec/step, loss=0.28892, avg_loss=0.28546]\n",
            "Step  280855 [5.359 sec/step, loss=0.27022, avg_loss=0.28525]\n",
            "Step  280856 [5.348 sec/step, loss=0.28669, avg_loss=0.28523]\n",
            "Step  280857 [4.909 sec/step, loss=0.28211, avg_loss=0.28528]\n",
            "Step  280858 [4.930 sec/step, loss=0.29360, avg_loss=0.28547]\n",
            "Step  280859 [4.936 sec/step, loss=0.28764, avg_loss=0.28544]\n",
            "Step  280860 [4.934 sec/step, loss=0.28569, avg_loss=0.28529]\n",
            "Step  280861 [4.940 sec/step, loss=0.27833, avg_loss=0.28522]\n",
            "Step  280862 [4.932 sec/step, loss=0.27988, avg_loss=0.28513]\n",
            "Step  280863 [4.944 sec/step, loss=0.29585, avg_loss=0.28523]\n",
            "Step  280864 [4.944 sec/step, loss=0.28802, avg_loss=0.28534]\n",
            "Step  280865 [4.938 sec/step, loss=0.27893, avg_loss=0.28525]\n",
            "Step  280866 [4.937 sec/step, loss=0.28479, avg_loss=0.28522]\n",
            "Step  280867 [4.956 sec/step, loss=0.28592, avg_loss=0.28524]\n",
            "Step  280868 [4.949 sec/step, loss=0.27212, avg_loss=0.28508]\n",
            "Step  280869 [4.949 sec/step, loss=0.28680, avg_loss=0.28502]\n",
            "Step  280870 [4.944 sec/step, loss=0.28392, avg_loss=0.28501]\n",
            "Step  280871 [4.933 sec/step, loss=0.26965, avg_loss=0.28472]\n",
            "Step  280872 [4.940 sec/step, loss=0.27304, avg_loss=0.28457]\n",
            "Step  280873 [4.950 sec/step, loss=0.29520, avg_loss=0.28463]\n",
            "Step  280874 [4.953 sec/step, loss=0.28564, avg_loss=0.28471]\n",
            "Step  280875 [4.951 sec/step, loss=0.26888, avg_loss=0.28452]\n",
            "Step  280876 [4.945 sec/step, loss=0.28248, avg_loss=0.28444]\n",
            "Step  280877 [4.954 sec/step, loss=0.28713, avg_loss=0.28446]\n",
            "Step  280878 [4.945 sec/step, loss=0.28351, avg_loss=0.28444]\n",
            "Step  280879 [4.947 sec/step, loss=0.28348, avg_loss=0.28441]\n",
            "Step  280880 [4.947 sec/step, loss=0.28385, avg_loss=0.28439]\n",
            "Step  280881 [4.952 sec/step, loss=0.28507, avg_loss=0.28439]\n",
            "Step  280882 [4.967 sec/step, loss=0.28444, avg_loss=0.28448]\n",
            "Step  280883 [4.970 sec/step, loss=0.28234, avg_loss=0.28452]\n",
            "Step  280884 [4.989 sec/step, loss=0.28514, avg_loss=0.28466]\n",
            "\n",
            "Generated 64 train batches of size 36 in 94.808 sec\n",
            "Step  280885 [5.522 sec/step, loss=0.28760, avg_loss=0.28469]\n",
            "Step  280886 [5.523 sec/step, loss=0.29449, avg_loss=0.28466]\n",
            "Step  280887 [5.513 sec/step, loss=0.27188, avg_loss=0.28439]\n",
            "Step  280888 [5.511 sec/step, loss=0.27174, avg_loss=0.28421]\n",
            "Step  280889 [5.498 sec/step, loss=0.26741, avg_loss=0.28403]\n",
            "Step  280890 [5.501 sec/step, loss=0.28358, avg_loss=0.28400]\n",
            "Step  280891 [5.496 sec/step, loss=0.26932, avg_loss=0.28382]\n",
            "Step  280892 [5.476 sec/step, loss=0.28412, avg_loss=0.28376]\n",
            "Step  280893 [5.480 sec/step, loss=0.28359, avg_loss=0.28366]\n",
            "Step  280894 [5.464 sec/step, loss=0.28140, avg_loss=0.28348]\n",
            "Step  280895 [5.456 sec/step, loss=0.28748, avg_loss=0.28352]\n",
            "Step  280896 [5.456 sec/step, loss=0.26827, avg_loss=0.28328]\n",
            "Step  280897 [5.449 sec/step, loss=0.27187, avg_loss=0.28309]\n",
            "Step  280898 [5.450 sec/step, loss=0.27235, avg_loss=0.28295]\n",
            "Step  280899 [5.447 sec/step, loss=0.27949, avg_loss=0.28301]\n",
            "Step  280900 [5.442 sec/step, loss=0.27741, avg_loss=0.28280]\n",
            "Step  280901 [5.442 sec/step, loss=0.27121, avg_loss=0.28276]\n",
            "Step  280902 [5.440 sec/step, loss=0.28727, avg_loss=0.28270]\n",
            "Step  280903 [5.434 sec/step, loss=0.28075, avg_loss=0.28261]\n",
            "Step  280904 [5.437 sec/step, loss=0.29469, avg_loss=0.28279]\n",
            "Step  280905 [5.433 sec/step, loss=0.28153, avg_loss=0.28272]\n",
            "Step  280906 [5.429 sec/step, loss=0.28687, avg_loss=0.28267]\n",
            "Step  280907 [5.433 sec/step, loss=0.28718, avg_loss=0.28270]\n",
            "Step  280908 [5.442 sec/step, loss=0.28350, avg_loss=0.28279]\n",
            "Step  280909 [5.446 sec/step, loss=0.28038, avg_loss=0.28275]\n",
            "Step  280910 [5.451 sec/step, loss=0.28331, avg_loss=0.28272]\n",
            "Step  280911 [5.451 sec/step, loss=0.28610, avg_loss=0.28271]\n",
            "Step  280912 [5.450 sec/step, loss=0.28097, avg_loss=0.28267]\n",
            "Step  280913 [5.452 sec/step, loss=0.28279, avg_loss=0.28266]\n",
            "Step  280914 [5.455 sec/step, loss=0.29419, avg_loss=0.28275]\n",
            "Step  280915 [5.459 sec/step, loss=0.28338, avg_loss=0.28268]\n",
            "Step  280916 [5.439 sec/step, loss=0.26793, avg_loss=0.28238]\n",
            "Step  280917 [5.429 sec/step, loss=0.28010, avg_loss=0.28235]\n",
            "Step  280918 [5.422 sec/step, loss=0.28292, avg_loss=0.28235]\n",
            "Step  280919 [5.427 sec/step, loss=0.28161, avg_loss=0.28244]\n",
            "Step  280920 [5.427 sec/step, loss=0.27868, avg_loss=0.28232]\n",
            "Step  280921 [4.906 sec/step, loss=0.28644, avg_loss=0.28229]\n",
            "Step  280922 [4.896 sec/step, loss=0.28136, avg_loss=0.28212]\n",
            "Step  280923 [4.895 sec/step, loss=0.27067, avg_loss=0.28200]\n",
            "Step  280924 [4.904 sec/step, loss=0.29121, avg_loss=0.28208]\n",
            "Step  280925 [4.897 sec/step, loss=0.26967, avg_loss=0.28196]\n",
            "Step  280926 [4.901 sec/step, loss=0.28224, avg_loss=0.28189]\n",
            "Step  280927 [4.900 sec/step, loss=0.27938, avg_loss=0.28186]\n",
            "Step  280928 [4.894 sec/step, loss=0.28644, avg_loss=0.28192]\n",
            "Step  280929 [4.892 sec/step, loss=0.28035, avg_loss=0.28186]\n",
            "Step  280930 [4.886 sec/step, loss=0.28552, avg_loss=0.28185]\n",
            "Step  280931 [4.875 sec/step, loss=0.27962, avg_loss=0.28184]\n",
            "Step  280932 [4.875 sec/step, loss=0.27987, avg_loss=0.28175]\n",
            "Step  280933 [4.874 sec/step, loss=0.28035, avg_loss=0.28166]\n",
            "Step  280934 [4.875 sec/step, loss=0.28205, avg_loss=0.28175]\n",
            "Step  280935 [4.880 sec/step, loss=0.29369, avg_loss=0.28199]\n",
            "Step  280936 [4.881 sec/step, loss=0.28180, avg_loss=0.28190]\n",
            "Step  280937 [4.884 sec/step, loss=0.27639, avg_loss=0.28192]\n",
            "Step  280938 [4.879 sec/step, loss=0.26819, avg_loss=0.28176]\n",
            "Step  280939 [4.876 sec/step, loss=0.27943, avg_loss=0.28159]\n",
            "Step  280940 [4.889 sec/step, loss=0.28188, avg_loss=0.28152]\n",
            "Step  280941 [4.900 sec/step, loss=0.28463, avg_loss=0.28153]\n",
            "Step  280942 [4.921 sec/step, loss=0.29340, avg_loss=0.28171]\n",
            "Step  280943 [4.935 sec/step, loss=0.28580, avg_loss=0.28185]\n",
            "Step  280944 [4.927 sec/step, loss=0.27971, avg_loss=0.28179]\n",
            "Step  280945 [4.928 sec/step, loss=0.27988, avg_loss=0.28174]\n",
            "Step  280946 [4.934 sec/step, loss=0.27843, avg_loss=0.28179]\n",
            "Step  280947 [4.939 sec/step, loss=0.27840, avg_loss=0.28173]\n",
            "Step  280948 [4.938 sec/step, loss=0.26722, avg_loss=0.28165]\n",
            "\n",
            "Generated 64 train batches of size 36 in 91.361 sec\n",
            "Step  280949 [5.414 sec/step, loss=0.27957, avg_loss=0.28167]\n",
            "Step  280950 [5.431 sec/step, loss=0.28163, avg_loss=0.28164]\n",
            "Step  280951 [5.422 sec/step, loss=0.26732, avg_loss=0.28159]\n",
            "Step  280952 [5.413 sec/step, loss=0.28119, avg_loss=0.28143]\n",
            "Step  280953 [5.410 sec/step, loss=0.27938, avg_loss=0.28140]\n",
            "Step  280954 [5.416 sec/step, loss=0.29018, avg_loss=0.28141]\n",
            "Step  280955 [5.432 sec/step, loss=0.27554, avg_loss=0.28146]\n",
            "Step  280956 [5.431 sec/step, loss=0.28455, avg_loss=0.28144]\n",
            "Step  280957 [5.425 sec/step, loss=0.27850, avg_loss=0.28141]\n",
            "Step  280958 [5.420 sec/step, loss=0.28064, avg_loss=0.28128]\n",
            "Step  280959 [5.409 sec/step, loss=0.26659, avg_loss=0.28107]\n",
            "Step  280960 [5.396 sec/step, loss=0.28032, avg_loss=0.28101]\n",
            "Step  280961 [5.402 sec/step, loss=0.28115, avg_loss=0.28104]\n",
            "Step  280962 [5.395 sec/step, loss=0.27901, avg_loss=0.28103]\n",
            "Step  280963 [5.388 sec/step, loss=0.27959, avg_loss=0.28087]\n",
            "Step  280964 [5.386 sec/step, loss=0.28385, avg_loss=0.28083]\n",
            "Step  280965 [5.383 sec/step, loss=0.28544, avg_loss=0.28089]\n",
            "Step  280966 [5.375 sec/step, loss=0.27059, avg_loss=0.28075]\n",
            "Step  280967 [5.371 sec/step, loss=0.28042, avg_loss=0.28070]\n",
            "Step  280968 [5.371 sec/step, loss=0.27876, avg_loss=0.28076]\n",
            "Step  280969 [5.369 sec/step, loss=0.27874, avg_loss=0.28068]\n",
            "Step  280970 [5.382 sec/step, loss=0.27744, avg_loss=0.28062]\n",
            "Step  280971 [5.387 sec/step, loss=0.26639, avg_loss=0.28058]\n",
            "Step  280972 [5.385 sec/step, loss=0.28476, avg_loss=0.28070]\n",
            "Step  280973 [5.370 sec/step, loss=0.28330, avg_loss=0.28058]\n",
            "Step  280974 [5.379 sec/step, loss=0.28048, avg_loss=0.28053]\n",
            "Step  280975 [5.381 sec/step, loss=0.26947, avg_loss=0.28054]\n",
            "Step  280976 [5.369 sec/step, loss=0.26557, avg_loss=0.28037]\n",
            "Step  280977 [5.368 sec/step, loss=0.27447, avg_loss=0.28024]\n",
            "Step  280978 [5.381 sec/step, loss=0.28053, avg_loss=0.28021]\n",
            "Step  280979 [5.379 sec/step, loss=0.28435, avg_loss=0.28022]\n",
            "Step  280980 [5.380 sec/step, loss=0.27661, avg_loss=0.28015]\n",
            "Step  280981 [5.374 sec/step, loss=0.28039, avg_loss=0.28010]\n",
            "Step  280982 [5.359 sec/step, loss=0.27895, avg_loss=0.28005]\n",
            "Step  280983 [5.362 sec/step, loss=0.28410, avg_loss=0.28006]\n",
            "Step  280984 [5.349 sec/step, loss=0.26947, avg_loss=0.27991]\n",
            "Step  280985 [4.808 sec/step, loss=0.26861, avg_loss=0.27972]\n",
            "Step  280986 [4.792 sec/step, loss=0.26342, avg_loss=0.27941]\n",
            "Step  280987 [4.798 sec/step, loss=0.27599, avg_loss=0.27945]\n",
            "Step  280988 [4.805 sec/step, loss=0.28368, avg_loss=0.27957]\n",
            "Step  280989 [4.815 sec/step, loss=0.29346, avg_loss=0.27983]\n",
            "Step  280990 [4.809 sec/step, loss=0.29390, avg_loss=0.27993]\n",
            "Step  280991 [4.826 sec/step, loss=0.27967, avg_loss=0.28003]\n",
            "Step  280992 [4.832 sec/step, loss=0.27801, avg_loss=0.27997]\n",
            "Step  280993 [4.832 sec/step, loss=0.27537, avg_loss=0.27989]\n",
            "Step  280994 [4.842 sec/step, loss=0.28288, avg_loss=0.27990]\n",
            "Step  280995 [4.846 sec/step, loss=0.28031, avg_loss=0.27983]\n",
            "Step  280996 [4.846 sec/step, loss=0.28354, avg_loss=0.27999]\n",
            "Step  280997 [4.858 sec/step, loss=0.29320, avg_loss=0.28020]\n",
            "Step  280998 [4.869 sec/step, loss=0.29229, avg_loss=0.28040]\n",
            "Step  280999 [4.879 sec/step, loss=0.29213, avg_loss=0.28052]\n",
            "Step  281000 [4.867 sec/step, loss=0.26476, avg_loss=0.28040]\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 281000: by a country that's been swept up by protests. i can barely imagine how head spinning these last few months have been for you. just as winner was.~\n",
            "Step  281001 [4.875 sec/step, loss=0.27816, avg_loss=0.28047]\n",
            "Step  281002 [4.871 sec/step, loss=0.27706, avg_loss=0.28037]\n",
            "Step  281003 [4.877 sec/step, loss=0.27810, avg_loss=0.28034]\n",
            "Step  281004 [4.875 sec/step, loss=0.26941, avg_loss=0.28009]\n",
            "Step  281005 [4.885 sec/step, loss=0.27444, avg_loss=0.28002]\n",
            "Step  281006 [4.893 sec/step, loss=0.28059, avg_loss=0.27995]\n",
            "Step  281007 [4.896 sec/step, loss=0.28232, avg_loss=0.27990]\n",
            "Step  281008 [4.905 sec/step, loss=0.28045, avg_loss=0.27987]\n",
            "Step  281009 [4.906 sec/step, loss=0.27680, avg_loss=0.27984]\n",
            "Step  281010 [4.904 sec/step, loss=0.26864, avg_loss=0.27969]\n",
            "Step  281011 [4.909 sec/step, loss=0.27807, avg_loss=0.27961]\n",
            "\n",
            "Generated 64 train batches of size 36 in 76.134 sec\n",
            "Step  281012 [5.256 sec/step, loss=0.27911, avg_loss=0.27959]\n",
            "Step  281013 [5.253 sec/step, loss=0.26757, avg_loss=0.27944]\n",
            "Step  281014 [5.246 sec/step, loss=0.26896, avg_loss=0.27919]\n",
            "Step  281015 [5.245 sec/step, loss=0.27934, avg_loss=0.27915]\n",
            "Step  281016 [5.255 sec/step, loss=0.27969, avg_loss=0.27926]\n",
            "Step  281017 [5.260 sec/step, loss=0.28276, avg_loss=0.27929]\n",
            "Step  281018 [5.270 sec/step, loss=0.28974, avg_loss=0.27936]\n",
            "Step  281019 [5.282 sec/step, loss=0.28975, avg_loss=0.27944]\n",
            "Step  281020 [5.287 sec/step, loss=0.27879, avg_loss=0.27944]\n",
            "Step  281021 [5.280 sec/step, loss=0.27310, avg_loss=0.27931]\n",
            "Step  281022 [5.283 sec/step, loss=0.26789, avg_loss=0.27917]\n",
            "Step  281023 [5.286 sec/step, loss=0.27601, avg_loss=0.27923]\n",
            "Step  281024 [5.284 sec/step, loss=0.28270, avg_loss=0.27914]\n",
            "Step  281025 [5.289 sec/step, loss=0.27575, avg_loss=0.27920]\n",
            "Step  281026 [5.286 sec/step, loss=0.27741, avg_loss=0.27916]\n",
            "Step  281027 [5.284 sec/step, loss=0.26708, avg_loss=0.27903]\n",
            "Step  281028 [5.289 sec/step, loss=0.27906, avg_loss=0.27896]\n",
            "Step  281029 [5.288 sec/step, loss=0.27418, avg_loss=0.27890]\n",
            "Step  281030 [5.291 sec/step, loss=0.27704, avg_loss=0.27881]\n",
            "Step  281031 [5.309 sec/step, loss=0.27843, avg_loss=0.27880]\n",
            "Step  281032 [5.317 sec/step, loss=0.27443, avg_loss=0.27875]\n",
            "Step  281033 [5.329 sec/step, loss=0.28945, avg_loss=0.27884]\n",
            "Step  281034 [5.322 sec/step, loss=0.27747, avg_loss=0.27879]\n",
            "Step  281035 [5.320 sec/step, loss=0.27837, avg_loss=0.27864]\n",
            "Step  281036 [5.325 sec/step, loss=0.27854, avg_loss=0.27860]\n",
            "Step  281037 [5.325 sec/step, loss=0.27641, avg_loss=0.27861]\n",
            "Step  281038 [5.327 sec/step, loss=0.27733, avg_loss=0.27870]\n",
            "Step  281039 [5.333 sec/step, loss=0.27770, avg_loss=0.27868]\n",
            "Step  281040 [5.320 sec/step, loss=0.27663, avg_loss=0.27863]\n",
            "Step  281041 [5.310 sec/step, loss=0.28141, avg_loss=0.27859]\n",
            "Step  281042 [5.303 sec/step, loss=0.28264, avg_loss=0.27849]\n",
            "Step  281043 [5.293 sec/step, loss=0.26634, avg_loss=0.27829]\n",
            "Step  281044 [5.280 sec/step, loss=0.26666, avg_loss=0.27816]\n",
            "Step  281045 [5.282 sec/step, loss=0.28201, avg_loss=0.27818]\n",
            "Step  281046 [5.271 sec/step, loss=0.27434, avg_loss=0.27814]\n",
            "Step  281047 [5.273 sec/step, loss=0.28733, avg_loss=0.27823]\n",
            "Step  281048 [5.279 sec/step, loss=0.27389, avg_loss=0.27830]\n",
            "Step  281049 [4.805 sec/step, loss=0.26578, avg_loss=0.27816]\n",
            "Step  281050 [4.802 sec/step, loss=0.27749, avg_loss=0.27812]\n",
            "Step  281051 [4.815 sec/step, loss=0.28095, avg_loss=0.27826]\n",
            "Step  281052 [4.817 sec/step, loss=0.28184, avg_loss=0.27826]\n",
            "Step  281053 [4.814 sec/step, loss=0.26810, avg_loss=0.27815]\n",
            "Step  281054 [4.800 sec/step, loss=0.27572, avg_loss=0.27800]\n",
            "Step  281055 [4.797 sec/step, loss=0.26603, avg_loss=0.27791]\n",
            "Step  281056 [4.784 sec/step, loss=0.26451, avg_loss=0.27771]\n",
            "Step  281057 [4.794 sec/step, loss=0.28948, avg_loss=0.27782]\n",
            "Step  281058 [4.789 sec/step, loss=0.27590, avg_loss=0.27777]\n",
            "Step  281059 [4.793 sec/step, loss=0.27556, avg_loss=0.27786]\n",
            "Step  281060 [4.791 sec/step, loss=0.27624, avg_loss=0.27782]\n",
            "Step  281061 [4.784 sec/step, loss=0.28197, avg_loss=0.27783]\n",
            "Step  281062 [4.797 sec/step, loss=0.27772, avg_loss=0.27782]\n",
            "Step  281063 [4.803 sec/step, loss=0.27775, avg_loss=0.27780]\n",
            "Step  281064 [4.812 sec/step, loss=0.27514, avg_loss=0.27771]\n",
            "Step  281065 [4.814 sec/step, loss=0.27501, avg_loss=0.27761]\n",
            "Step  281066 [4.824 sec/step, loss=0.27433, avg_loss=0.27764]\n",
            "Step  281067 [4.834 sec/step, loss=0.27387, avg_loss=0.27758]\n",
            "Step  281068 [4.847 sec/step, loss=0.28029, avg_loss=0.27759]\n",
            "Step  281069 [4.848 sec/step, loss=0.27535, avg_loss=0.27756]\n",
            "Step  281070 [4.838 sec/step, loss=0.26429, avg_loss=0.27743]\n",
            "Step  281071 [4.838 sec/step, loss=0.26468, avg_loss=0.27741]\n",
            "Step  281072 [4.830 sec/step, loss=0.26191, avg_loss=0.27718]\n",
            "Step  281073 [4.837 sec/step, loss=0.28123, avg_loss=0.27716]\n",
            "Step  281074 [4.838 sec/step, loss=0.28927, avg_loss=0.27725]\n",
            "Step  281075 [4.841 sec/step, loss=0.28073, avg_loss=0.27736]\n",
            "\n",
            "Generated 64 train batches of size 36 in 80.442 sec\n",
            "Step  281076 [5.252 sec/step, loss=0.27710, avg_loss=0.27748]\n",
            "Step  281077 [5.245 sec/step, loss=0.27361, avg_loss=0.27747]\n",
            "Step  281078 [5.232 sec/step, loss=0.27780, avg_loss=0.27744]\n",
            "Step  281079 [5.238 sec/step, loss=0.28849, avg_loss=0.27748]\n",
            "Step  281080 [5.233 sec/step, loss=0.26660, avg_loss=0.27738]\n",
            "Step  281081 [5.245 sec/step, loss=0.27117, avg_loss=0.27729]\n",
            "Step  281082 [5.246 sec/step, loss=0.26547, avg_loss=0.27716]\n",
            "Step  281083 [5.245 sec/step, loss=0.26416, avg_loss=0.27696]\n",
            "Step  281084 [5.243 sec/step, loss=0.27500, avg_loss=0.27701]\n",
            "Step  281085 [5.241 sec/step, loss=0.28096, avg_loss=0.27714]\n",
            "Step  281086 [5.252 sec/step, loss=0.27730, avg_loss=0.27727]\n",
            "Step  281087 [5.258 sec/step, loss=0.27951, avg_loss=0.27731]\n",
            "Step  281088 [5.255 sec/step, loss=0.26537, avg_loss=0.27713]\n",
            "Step  281089 [5.250 sec/step, loss=0.26506, avg_loss=0.27684]\n",
            "Step  281090 [5.237 sec/step, loss=0.27450, avg_loss=0.27665]\n",
            "Step  281091 [5.233 sec/step, loss=0.27817, avg_loss=0.27663]\n",
            "Step  281092 [5.231 sec/step, loss=0.26401, avg_loss=0.27649]\n",
            "Step  281093 [5.224 sec/step, loss=0.27256, avg_loss=0.27646]\n",
            "Step  281094 [5.209 sec/step, loss=0.26427, avg_loss=0.27628]\n",
            "Step  281095 [5.209 sec/step, loss=0.27975, avg_loss=0.27627]\n",
            "Step  281096 [5.218 sec/step, loss=0.27648, avg_loss=0.27620]\n",
            "Step  281097 [5.210 sec/step, loss=0.26181, avg_loss=0.27589]\n",
            "Step  281098 [5.206 sec/step, loss=0.28006, avg_loss=0.27577]\n",
            "Step  281099 [5.207 sec/step, loss=0.28069, avg_loss=0.27565]\n",
            "Step  281100 [5.209 sec/step, loss=0.26261, avg_loss=0.27563]\n",
            "Step  281101 [5.206 sec/step, loss=0.28059, avg_loss=0.27565]\n",
            "Step  281102 [5.213 sec/step, loss=0.27407, avg_loss=0.27562]\n",
            "Step  281103 [5.206 sec/step, loss=0.26172, avg_loss=0.27546]\n",
            "Step  281104 [5.200 sec/step, loss=0.27214, avg_loss=0.27549]\n",
            "Step  281105 [5.204 sec/step, loss=0.27720, avg_loss=0.27552]\n",
            "Step  281106 [5.205 sec/step, loss=0.28861, avg_loss=0.27560]\n",
            "Step  281107 [5.196 sec/step, loss=0.27410, avg_loss=0.27551]\n",
            "Step  281108 [5.181 sec/step, loss=0.27844, avg_loss=0.27549]\n",
            "Step  281109 [5.176 sec/step, loss=0.27040, avg_loss=0.27543]\n",
            "Step  281110 [5.170 sec/step, loss=0.27340, avg_loss=0.27548]\n",
            "Step  281111 [5.164 sec/step, loss=0.25975, avg_loss=0.27529]\n",
            "Step  281112 [4.819 sec/step, loss=0.27324, avg_loss=0.27524]\n",
            "Step  281113 [4.819 sec/step, loss=0.27974, avg_loss=0.27536]\n",
            "Step  281114 [4.823 sec/step, loss=0.27602, avg_loss=0.27543]\n",
            "Step  281115 [4.821 sec/step, loss=0.27997, avg_loss=0.27543]\n",
            "Step  281116 [4.820 sec/step, loss=0.25797, avg_loss=0.27522]\n",
            "Step  281117 [4.829 sec/step, loss=0.27678, avg_loss=0.27516]\n",
            "Step  281118 [4.828 sec/step, loss=0.28828, avg_loss=0.27514]\n",
            "Step  281119 [4.825 sec/step, loss=0.27907, avg_loss=0.27504]\n",
            "Step  281120 [4.821 sec/step, loss=0.27671, avg_loss=0.27501]\n",
            "Step  281121 [4.824 sec/step, loss=0.27306, avg_loss=0.27501]\n",
            "Step  281122 [4.827 sec/step, loss=0.27583, avg_loss=0.27509]\n",
            "Step  281123 [4.827 sec/step, loss=0.27647, avg_loss=0.27510]\n",
            "Step  281124 [4.832 sec/step, loss=0.27847, avg_loss=0.27506]\n",
            "Step  281125 [4.843 sec/step, loss=0.28842, avg_loss=0.27518]\n",
            "Step  281126 [4.845 sec/step, loss=0.27213, avg_loss=0.27513]\n",
            "Step  281127 [4.855 sec/step, loss=0.27640, avg_loss=0.27522]\n",
            "Step  281128 [4.845 sec/step, loss=0.27515, avg_loss=0.27518]\n",
            "Step  281129 [4.844 sec/step, loss=0.27213, avg_loss=0.27516]\n",
            "Step  281130 [4.846 sec/step, loss=0.27504, avg_loss=0.27514]\n",
            "Step  281131 [4.832 sec/step, loss=0.27349, avg_loss=0.27509]\n",
            "Step  281132 [4.835 sec/step, loss=0.27605, avg_loss=0.27511]\n",
            "Step  281133 [4.826 sec/step, loss=0.27452, avg_loss=0.27496]\n",
            "Step  281134 [4.834 sec/step, loss=0.27343, avg_loss=0.27492]\n",
            "Step  281135 [4.839 sec/step, loss=0.27536, avg_loss=0.27489]\n",
            "Step  281136 [4.844 sec/step, loss=0.28783, avg_loss=0.27498]\n",
            "Step  281137 [4.841 sec/step, loss=0.26451, avg_loss=0.27486]\n",
            "Step  281138 [4.858 sec/step, loss=0.28584, avg_loss=0.27495]\n",
            "Step  281139 [4.858 sec/step, loss=0.27134, avg_loss=0.27489]\n",
            "\n",
            "Generated 64 train batches of size 36 in 86.556 sec\n",
            "Step  281140 [5.301 sec/step, loss=0.27246, avg_loss=0.27484]\n",
            "Step  281141 [5.311 sec/step, loss=0.27119, avg_loss=0.27474]\n",
            "Step  281142 [5.310 sec/step, loss=0.28689, avg_loss=0.27478]\n",
            "Step  281143 [5.314 sec/step, loss=0.27054, avg_loss=0.27483]\n",
            "Step  281144 [5.318 sec/step, loss=0.27162, avg_loss=0.27488]\n",
            "Step  281145 [5.315 sec/step, loss=0.27830, avg_loss=0.27484]\n",
            "Step  281146 [5.315 sec/step, loss=0.26078, avg_loss=0.27470]\n",
            "Step  281147 [5.307 sec/step, loss=0.27291, avg_loss=0.27456]\n",
            "Step  281148 [5.319 sec/step, loss=0.28605, avg_loss=0.27468]\n",
            "Step  281149 [5.320 sec/step, loss=0.27801, avg_loss=0.27480]\n",
            "Step  281150 [5.317 sec/step, loss=0.27333, avg_loss=0.27476]\n",
            "Step  281151 [5.307 sec/step, loss=0.27305, avg_loss=0.27468]\n",
            "Step  281152 [5.309 sec/step, loss=0.26943, avg_loss=0.27456]\n",
            "Step  281153 [5.316 sec/step, loss=0.27801, avg_loss=0.27466]\n",
            "Step  281154 [5.316 sec/step, loss=0.26477, avg_loss=0.27455]\n",
            "Step  281155 [5.308 sec/step, loss=0.27259, avg_loss=0.27461]\n",
            "Step  281156 [5.326 sec/step, loss=0.28488, avg_loss=0.27482]\n",
            "Step  281157 [5.315 sec/step, loss=0.27966, avg_loss=0.27472]\n",
            "Step  281158 [5.313 sec/step, loss=0.27095, avg_loss=0.27467]\n",
            "Step  281159 [5.313 sec/step, loss=0.26328, avg_loss=0.27455]\n",
            "Step  281160 [5.322 sec/step, loss=0.26802, avg_loss=0.27446]\n",
            "Step  281161 [5.311 sec/step, loss=0.26081, avg_loss=0.27425]\n",
            "Step  281162 [5.309 sec/step, loss=0.27647, avg_loss=0.27424]\n",
            "Step  281163 [5.304 sec/step, loss=0.26922, avg_loss=0.27416]\n",
            "Step  281164 [5.297 sec/step, loss=0.25969, avg_loss=0.27400]\n",
            "Step  281165 [5.305 sec/step, loss=0.27689, avg_loss=0.27402]\n",
            "Step  281166 [5.304 sec/step, loss=0.27626, avg_loss=0.27404]\n",
            "Step  281167 [5.287 sec/step, loss=0.27129, avg_loss=0.27401]\n",
            "Step  281168 [5.281 sec/step, loss=0.27518, avg_loss=0.27396]\n",
            "Step  281169 [5.280 sec/step, loss=0.27249, avg_loss=0.27393]\n",
            "Step  281170 [5.284 sec/step, loss=0.26997, avg_loss=0.27399]\n",
            "Step  281171 [5.287 sec/step, loss=0.27573, avg_loss=0.27410]\n",
            "Step  281172 [5.304 sec/step, loss=0.28468, avg_loss=0.27433]\n",
            "Step  281173 [5.299 sec/step, loss=0.27127, avg_loss=0.27423]\n",
            "Step  281174 [5.297 sec/step, loss=0.28332, avg_loss=0.27417]\n",
            "Step  281175 [5.291 sec/step, loss=0.26485, avg_loss=0.27401]\n",
            "Step  281176 [4.891 sec/step, loss=0.25856, avg_loss=0.27383]\n",
            "Step  281177 [4.899 sec/step, loss=0.27551, avg_loss=0.27384]\n",
            "Step  281178 [4.897 sec/step, loss=0.26459, avg_loss=0.27371]\n",
            "Step  281179 [4.901 sec/step, loss=0.27557, avg_loss=0.27358]\n",
            "Step  281180 [4.915 sec/step, loss=0.27468, avg_loss=0.27366]\n",
            "Step  281181 [4.913 sec/step, loss=0.27430, avg_loss=0.27370]\n",
            "Step  281182 [4.916 sec/step, loss=0.27753, avg_loss=0.27382]\n",
            "Step  281183 [4.918 sec/step, loss=0.27974, avg_loss=0.27397]\n",
            "Step  281184 [4.926 sec/step, loss=0.27941, avg_loss=0.27402]\n",
            "Step  281185 [4.923 sec/step, loss=0.26417, avg_loss=0.27385]\n",
            "Step  281186 [4.926 sec/step, loss=0.27477, avg_loss=0.27382]\n",
            "Step  281187 [4.927 sec/step, loss=0.26824, avg_loss=0.27371]\n",
            "Step  281188 [4.929 sec/step, loss=0.27233, avg_loss=0.27378]\n",
            "Step  281189 [4.931 sec/step, loss=0.26901, avg_loss=0.27382]\n",
            "Step  281190 [4.943 sec/step, loss=0.27349, avg_loss=0.27381]\n",
            "Step  281191 [4.942 sec/step, loss=0.27689, avg_loss=0.27380]\n",
            "Step  281192 [4.948 sec/step, loss=0.28423, avg_loss=0.27400]\n",
            "Step  281193 [4.947 sec/step, loss=0.25731, avg_loss=0.27385]\n",
            "Step  281194 [4.950 sec/step, loss=0.26276, avg_loss=0.27383]\n",
            "Step  281195 [4.947 sec/step, loss=0.25871, avg_loss=0.27362]\n",
            "Step  281196 [4.946 sec/step, loss=0.27379, avg_loss=0.27359]\n",
            "Step  281197 [4.955 sec/step, loss=0.27638, avg_loss=0.27374]\n",
            "Step  281198 [4.953 sec/step, loss=0.27788, avg_loss=0.27372]\n",
            "Step  281199 [4.956 sec/step, loss=0.27818, avg_loss=0.27369]\n",
            "Step  281200 [4.958 sec/step, loss=0.27219, avg_loss=0.27379]\n",
            "Step  281201 [4.958 sec/step, loss=0.27092, avg_loss=0.27369]\n",
            "Step  281202 [4.955 sec/step, loss=0.27183, avg_loss=0.27367]\n",
            "Step  281203 [4.958 sec/step, loss=0.26195, avg_loss=0.27367]\n",
            "\n",
            "Generated 64 train batches of size 36 in 82.558 sec\n",
            "Step  281204 [5.397 sec/step, loss=0.28380, avg_loss=0.27379]\n",
            "Step  281205 [5.391 sec/step, loss=0.27652, avg_loss=0.27378]\n",
            "Step  281206 [5.384 sec/step, loss=0.26827, avg_loss=0.27358]\n",
            "Step  281207 [5.398 sec/step, loss=0.28341, avg_loss=0.27367]\n",
            "Step  281208 [5.397 sec/step, loss=0.26165, avg_loss=0.27350]\n",
            "Step  281209 [5.397 sec/step, loss=0.27077, avg_loss=0.27351]\n",
            "Step  281210 [5.407 sec/step, loss=0.27230, avg_loss=0.27350]\n",
            "Step  281211 [5.415 sec/step, loss=0.26677, avg_loss=0.27357]\n",
            "Step  281212 [5.415 sec/step, loss=0.26105, avg_loss=0.27344]\n",
            "Step  281213 [5.421 sec/step, loss=0.27677, avg_loss=0.27341]\n",
            "Step  281214 [5.419 sec/step, loss=0.27622, avg_loss=0.27342]\n",
            "Step  281215 [5.414 sec/step, loss=0.25881, avg_loss=0.27320]\n",
            "Step  281216 [5.427 sec/step, loss=0.27571, avg_loss=0.27338]\n",
            "Step  281217 [5.413 sec/step, loss=0.26185, avg_loss=0.27323]\n",
            "Step  281218 [5.417 sec/step, loss=0.27521, avg_loss=0.27310]\n",
            "Step  281219 [5.398 sec/step, loss=0.27011, avg_loss=0.27301]\n",
            "Step  281220 [5.394 sec/step, loss=0.27032, avg_loss=0.27295]\n",
            "Step  281221 [5.398 sec/step, loss=0.27013, avg_loss=0.27292]\n",
            "Step  281222 [5.396 sec/step, loss=0.26941, avg_loss=0.27285]\n",
            "Step  281223 [5.395 sec/step, loss=0.27062, avg_loss=0.27280]\n",
            "Step  281224 [5.390 sec/step, loss=0.27417, avg_loss=0.27275]\n",
            "Step  281225 [5.387 sec/step, loss=0.27375, avg_loss=0.27261]\n",
            "Step  281226 [5.380 sec/step, loss=0.27648, avg_loss=0.27265]\n",
            "Step  281227 [5.367 sec/step, loss=0.25826, avg_loss=0.27247]\n",
            "Step  281228 [5.368 sec/step, loss=0.25991, avg_loss=0.27232]\n",
            "Step  281229 [5.366 sec/step, loss=0.27392, avg_loss=0.27233]\n",
            "Step  281230 [5.364 sec/step, loss=0.27457, avg_loss=0.27233]\n",
            "Step  281231 [5.368 sec/step, loss=0.27449, avg_loss=0.27234]\n",
            "Step  281232 [5.359 sec/step, loss=0.26818, avg_loss=0.27226]\n",
            "Step  281233 [5.351 sec/step, loss=0.26935, avg_loss=0.27221]\n",
            "Step  281234 [5.358 sec/step, loss=0.27316, avg_loss=0.27221]\n",
            "Step  281235 [5.357 sec/step, loss=0.28430, avg_loss=0.27230]\n",
            "Step  281236 [5.348 sec/step, loss=0.27599, avg_loss=0.27218]\n",
            "Step  281237 [5.355 sec/step, loss=0.27359, avg_loss=0.27227]\n",
            "Step  281238 [5.346 sec/step, loss=0.26993, avg_loss=0.27211]\n",
            "Step  281239 [5.345 sec/step, loss=0.26802, avg_loss=0.27208]\n",
            "Step  281240 [4.909 sec/step, loss=0.27103, avg_loss=0.27206]\n",
            "Step  281241 [4.903 sec/step, loss=0.27618, avg_loss=0.27211]\n",
            "Step  281242 [4.898 sec/step, loss=0.27583, avg_loss=0.27200]\n",
            "Step  281243 [4.900 sec/step, loss=0.26784, avg_loss=0.27197]\n",
            "Step  281244 [4.900 sec/step, loss=0.26849, avg_loss=0.27194]\n",
            "Step  281245 [4.908 sec/step, loss=0.28398, avg_loss=0.27200]\n",
            "Step  281246 [4.922 sec/step, loss=0.27302, avg_loss=0.27212]\n",
            "Step  281247 [4.918 sec/step, loss=0.27107, avg_loss=0.27210]\n",
            "Step  281248 [4.899 sec/step, loss=0.25577, avg_loss=0.27180]\n",
            "Step  281249 [4.900 sec/step, loss=0.27264, avg_loss=0.27175]\n",
            "Step  281250 [4.897 sec/step, loss=0.27087, avg_loss=0.27172]\n",
            "Step  281251 [4.896 sec/step, loss=0.25787, avg_loss=0.27157]\n",
            "Step  281252 [4.888 sec/step, loss=0.26112, avg_loss=0.27149]\n",
            "Step  281253 [4.881 sec/step, loss=0.25762, avg_loss=0.27128]\n",
            "Step  281254 [4.885 sec/step, loss=0.25713, avg_loss=0.27121]\n",
            "Step  281255 [4.897 sec/step, loss=0.28397, avg_loss=0.27132]\n",
            "Step  281256 [4.898 sec/step, loss=0.26607, avg_loss=0.27113]\n",
            "Step  281257 [4.914 sec/step, loss=0.27237, avg_loss=0.27106]\n",
            "Step  281258 [4.911 sec/step, loss=0.26921, avg_loss=0.27104]\n",
            "Step  281259 [4.923 sec/step, loss=0.26786, avg_loss=0.27109]\n",
            "Step  281260 [4.936 sec/step, loss=0.27258, avg_loss=0.27113]\n",
            "Step  281261 [4.944 sec/step, loss=0.26012, avg_loss=0.27113]\n",
            "Step  281262 [4.943 sec/step, loss=0.26776, avg_loss=0.27104]\n",
            "Step  281263 [4.942 sec/step, loss=0.26097, avg_loss=0.27096]\n",
            "Step  281264 [4.940 sec/step, loss=0.26928, avg_loss=0.27105]\n",
            "Step  281265 [4.948 sec/step, loss=0.28148, avg_loss=0.27110]\n",
            "Step  281266 [4.952 sec/step, loss=0.27246, avg_loss=0.27106]\n",
            "Step  281267 [4.962 sec/step, loss=0.27218, avg_loss=0.27107]\n",
            "\n",
            "Generated 64 train batches of size 36 in 85.620 sec\n",
            "Step  281268 [5.377 sec/step, loss=0.25919, avg_loss=0.27091]\n",
            "Step  281269 [5.372 sec/step, loss=0.25365, avg_loss=0.27072]\n",
            "Step  281270 [5.366 sec/step, loss=0.26960, avg_loss=0.27072]\n",
            "Step  281271 [5.361 sec/step, loss=0.26874, avg_loss=0.27065]\n",
            "Step  281272 [5.348 sec/step, loss=0.26867, avg_loss=0.27049]\n",
            "Step  281273 [5.348 sec/step, loss=0.26825, avg_loss=0.27046]\n",
            "Step  281274 [5.341 sec/step, loss=0.27227, avg_loss=0.27035]\n",
            "Step  281275 [5.344 sec/step, loss=0.26654, avg_loss=0.27036]\n",
            "Step  281276 [5.344 sec/step, loss=0.26876, avg_loss=0.27047]\n",
            "Step  281277 [5.347 sec/step, loss=0.27125, avg_loss=0.27042]\n",
            "Step  281278 [5.354 sec/step, loss=0.27203, avg_loss=0.27050]\n",
            "Step  281279 [5.348 sec/step, loss=0.28228, avg_loss=0.27056]\n",
            "Step  281280 [5.343 sec/step, loss=0.27150, avg_loss=0.27053]\n",
            "Step  281281 [5.338 sec/step, loss=0.26601, avg_loss=0.27045]\n",
            "Step  281282 [5.341 sec/step, loss=0.27517, avg_loss=0.27043]\n",
            "Step  281283 [5.332 sec/step, loss=0.26687, avg_loss=0.27030]\n",
            "Step  281284 [5.334 sec/step, loss=0.27083, avg_loss=0.27021]\n",
            "Step  281285 [5.343 sec/step, loss=0.26584, avg_loss=0.27023]\n",
            "Step  281286 [5.336 sec/step, loss=0.26679, avg_loss=0.27015]\n",
            "Step  281287 [5.333 sec/step, loss=0.27571, avg_loss=0.27022]\n",
            "Step  281288 [5.325 sec/step, loss=0.25631, avg_loss=0.27006]\n",
            "Step  281289 [5.329 sec/step, loss=0.28249, avg_loss=0.27020]\n",
            "Step  281290 [5.319 sec/step, loss=0.26719, avg_loss=0.27014]\n",
            "Step  281291 [5.317 sec/step, loss=0.26501, avg_loss=0.27002]\n",
            "Step  281292 [5.310 sec/step, loss=0.27065, avg_loss=0.26988]\n",
            "Step  281293 [5.314 sec/step, loss=0.27084, avg_loss=0.27002]\n",
            "Step  281294 [5.324 sec/step, loss=0.26995, avg_loss=0.27009]\n",
            "Step  281295 [5.329 sec/step, loss=0.26484, avg_loss=0.27015]\n",
            "Step  281296 [5.322 sec/step, loss=0.25985, avg_loss=0.27001]\n",
            "Step  281297 [5.327 sec/step, loss=0.27458, avg_loss=0.26999]\n",
            "Step  281298 [5.321 sec/step, loss=0.25581, avg_loss=0.26977]\n",
            "Step  281299 [5.310 sec/step, loss=0.25984, avg_loss=0.26959]\n",
            "Step  281300 [5.309 sec/step, loss=0.27533, avg_loss=0.26962]\n",
            "Step  281301 [5.308 sec/step, loss=0.26722, avg_loss=0.26958]\n",
            "Step  281302 [5.305 sec/step, loss=0.25934, avg_loss=0.26946]\n",
            "Step  281303 [5.317 sec/step, loss=0.28162, avg_loss=0.26965]\n",
            "Step  281304 [4.882 sec/step, loss=0.26817, avg_loss=0.26950]\n",
            "Step  281305 [4.880 sec/step, loss=0.25453, avg_loss=0.26928]\n",
            "Step  281306 [4.876 sec/step, loss=0.25927, avg_loss=0.26919]\n",
            "Step  281307 [4.868 sec/step, loss=0.27791, avg_loss=0.26913]\n",
            "Step  281308 [4.871 sec/step, loss=0.27493, avg_loss=0.26927]\n",
            "Step  281309 [4.880 sec/step, loss=0.28109, avg_loss=0.26937]\n",
            "Step  281310 [4.876 sec/step, loss=0.27401, avg_loss=0.26939]\n",
            "Step  281311 [4.876 sec/step, loss=0.27043, avg_loss=0.26942]\n",
            "Step  281312 [4.877 sec/step, loss=0.26729, avg_loss=0.26948]\n",
            "Step  281313 [4.876 sec/step, loss=0.27365, avg_loss=0.26945]\n",
            "Step  281314 [4.873 sec/step, loss=0.26683, avg_loss=0.26936]\n",
            "Step  281315 [4.875 sec/step, loss=0.26788, avg_loss=0.26945]\n",
            "Step  281316 [4.874 sec/step, loss=0.27309, avg_loss=0.26942]\n",
            "Step  281317 [4.883 sec/step, loss=0.26971, avg_loss=0.26950]\n",
            "Step  281318 [4.874 sec/step, loss=0.27033, avg_loss=0.26945]\n",
            "Step  281319 [4.883 sec/step, loss=0.27358, avg_loss=0.26949]\n",
            "Step  281320 [4.877 sec/step, loss=0.25448, avg_loss=0.26933]\n",
            "Step  281321 [4.867 sec/step, loss=0.25450, avg_loss=0.26917]\n",
            "Step  281322 [4.869 sec/step, loss=0.26950, avg_loss=0.26917]\n",
            "Step  281323 [4.867 sec/step, loss=0.26807, avg_loss=0.26915]\n",
            "Step  281324 [4.860 sec/step, loss=0.25863, avg_loss=0.26899]\n",
            "Step  281325 [4.856 sec/step, loss=0.26619, avg_loss=0.26892]\n",
            "Step  281326 [4.864 sec/step, loss=0.26533, avg_loss=0.26881]\n",
            "Step  281327 [4.870 sec/step, loss=0.26395, avg_loss=0.26886]\n",
            "Step  281328 [4.875 sec/step, loss=0.25820, avg_loss=0.26885]\n",
            "Step  281329 [4.888 sec/step, loss=0.28102, avg_loss=0.26892]\n",
            "Step  281330 [4.890 sec/step, loss=0.27423, avg_loss=0.26891]\n",
            "Step  281331 [4.889 sec/step, loss=0.27337, avg_loss=0.26890]\n",
            "\n",
            "Generated 64 train batches of size 36 in 96.569 sec\n",
            "Step  281332 [5.443 sec/step, loss=0.25457, avg_loss=0.26877]\n",
            "Step  281333 [5.449 sec/step, loss=0.26471, avg_loss=0.26872]\n",
            "Step  281334 [5.454 sec/step, loss=0.27942, avg_loss=0.26878]\n",
            "Step  281335 [5.450 sec/step, loss=0.26411, avg_loss=0.26858]\n",
            "Step  281336 [5.451 sec/step, loss=0.27997, avg_loss=0.26862]\n",
            "Step  281337 [5.448 sec/step, loss=0.25732, avg_loss=0.26846]\n",
            "Step  281338 [5.452 sec/step, loss=0.26978, avg_loss=0.26846]\n",
            "Step  281339 [5.441 sec/step, loss=0.26695, avg_loss=0.26845]\n",
            "Step  281340 [5.439 sec/step, loss=0.26953, avg_loss=0.26843]\n",
            "Step  281341 [5.441 sec/step, loss=0.26869, avg_loss=0.26836]\n",
            "Step  281342 [5.443 sec/step, loss=0.26405, avg_loss=0.26824]\n",
            "Step  281343 [5.433 sec/step, loss=0.25370, avg_loss=0.26810]\n",
            "Step  281344 [5.441 sec/step, loss=0.26923, avg_loss=0.26810]\n",
            "Step  281345 [5.441 sec/step, loss=0.26669, avg_loss=0.26793]\n",
            "Step  281346 [5.442 sec/step, loss=0.27949, avg_loss=0.26800]\n",
            "Step  281347 [5.444 sec/step, loss=0.26705, avg_loss=0.26796]\n",
            "Step  281348 [5.443 sec/step, loss=0.25409, avg_loss=0.26794]\n",
            "Step  281349 [5.435 sec/step, loss=0.25290, avg_loss=0.26774]\n",
            "Step  281350 [5.444 sec/step, loss=0.26953, avg_loss=0.26773]\n",
            "Step  281351 [5.447 sec/step, loss=0.25685, avg_loss=0.26772]\n",
            "Step  281352 [5.461 sec/step, loss=0.27908, avg_loss=0.26790]\n",
            "Step  281353 [5.466 sec/step, loss=0.26445, avg_loss=0.26797]\n",
            "Step  281354 [5.475 sec/step, loss=0.26908, avg_loss=0.26808]\n",
            "Step  281355 [5.474 sec/step, loss=0.26731, avg_loss=0.26792]\n",
            "Step  281356 [5.466 sec/step, loss=0.26630, avg_loss=0.26792]\n",
            "Step  281357 [5.462 sec/step, loss=0.26893, avg_loss=0.26789]\n",
            "Step  281358 [5.463 sec/step, loss=0.26754, avg_loss=0.26787]\n",
            "Step  281359 [5.452 sec/step, loss=0.26674, avg_loss=0.26786]\n",
            "Step  281360 [5.435 sec/step, loss=0.26589, avg_loss=0.26779]\n",
            "Step  281361 [5.441 sec/step, loss=0.27220, avg_loss=0.26791]\n",
            "Step  281362 [5.431 sec/step, loss=0.25823, avg_loss=0.26782]\n",
            "Step  281363 [5.440 sec/step, loss=0.27872, avg_loss=0.26799]\n",
            "Step  281364 [5.444 sec/step, loss=0.27377, avg_loss=0.26804]\n",
            "Step  281365 [5.438 sec/step, loss=0.26874, avg_loss=0.26791]\n",
            "Step  281366 [5.436 sec/step, loss=0.26843, avg_loss=0.26787]\n",
            "Step  281367 [5.439 sec/step, loss=0.27853, avg_loss=0.26794]\n",
            "Step  281368 [5.031 sec/step, loss=0.26847, avg_loss=0.26803]\n",
            "Step  281369 [5.045 sec/step, loss=0.26646, avg_loss=0.26816]\n",
            "Step  281370 [5.046 sec/step, loss=0.26568, avg_loss=0.26812]\n",
            "Step  281371 [5.047 sec/step, loss=0.25717, avg_loss=0.26800]\n",
            "Step  281372 [5.050 sec/step, loss=0.27277, avg_loss=0.26804]\n",
            "Step  281373 [5.054 sec/step, loss=0.27266, avg_loss=0.26809]\n",
            "Step  281374 [5.053 sec/step, loss=0.26772, avg_loss=0.26804]\n",
            "Step  281375 [5.059 sec/step, loss=0.27333, avg_loss=0.26811]\n",
            "Step  281376 [5.058 sec/step, loss=0.27230, avg_loss=0.26814]\n",
            "Step  281377 [5.043 sec/step, loss=0.25716, avg_loss=0.26800]\n",
            "Step  281378 [5.043 sec/step, loss=0.27259, avg_loss=0.26801]\n",
            "Step  281379 [5.043 sec/step, loss=0.27052, avg_loss=0.26789]\n",
            "Step  281380 [5.032 sec/step, loss=0.25675, avg_loss=0.26774]\n",
            "Step  281381 [5.026 sec/step, loss=0.26522, avg_loss=0.26774]\n",
            "Step  281382 [5.028 sec/step, loss=0.26510, avg_loss=0.26764]\n",
            "Step  281383 [5.026 sec/step, loss=0.26654, avg_loss=0.26763]\n",
            "Step  281384 [5.019 sec/step, loss=0.26396, avg_loss=0.26756]\n",
            "Step  281385 [5.011 sec/step, loss=0.25377, avg_loss=0.26744]\n",
            "Step  281386 [5.007 sec/step, loss=0.27131, avg_loss=0.26749]\n",
            "Step  281387 [5.014 sec/step, loss=0.26781, avg_loss=0.26741]\n",
            "Step  281388 [5.020 sec/step, loss=0.27124, avg_loss=0.26756]\n",
            "Step  281389 [5.016 sec/step, loss=0.26408, avg_loss=0.26737]\n",
            "Step  281390 [5.020 sec/step, loss=0.26612, avg_loss=0.26736]\n",
            "Step  281391 [5.016 sec/step, loss=0.26457, avg_loss=0.26736]\n",
            "Step  281392 [5.015 sec/step, loss=0.25606, avg_loss=0.26721]\n",
            "Step  281393 [5.019 sec/step, loss=0.27091, avg_loss=0.26721]\n",
            "Step  281394 [5.012 sec/step, loss=0.25074, avg_loss=0.26702]\n",
            "Step  281395 [5.012 sec/step, loss=0.26390, avg_loss=0.26701]\n",
            "\n",
            "Generated 64 train batches of size 36 in 94.624 sec\n",
            "Step  281396 [5.562 sec/step, loss=0.26429, avg_loss=0.26706]\n",
            "Step  281397 [5.550 sec/step, loss=0.25656, avg_loss=0.26688]\n",
            "Step  281398 [5.546 sec/step, loss=0.25218, avg_loss=0.26684]\n",
            "Step  281399 [5.561 sec/step, loss=0.27889, avg_loss=0.26703]\n",
            "Step  281400 [5.556 sec/step, loss=0.25069, avg_loss=0.26678]\n",
            "Step  281401 [5.560 sec/step, loss=0.27034, avg_loss=0.26682]\n",
            "Step  281402 [5.570 sec/step, loss=0.26752, avg_loss=0.26690]\n",
            "Step  281403 [5.551 sec/step, loss=0.25540, avg_loss=0.26663]\n",
            "Step  281404 [5.549 sec/step, loss=0.26458, avg_loss=0.26660]\n",
            "Step  281405 [5.553 sec/step, loss=0.26494, avg_loss=0.26670]\n",
            "Step  281406 [5.554 sec/step, loss=0.25497, avg_loss=0.26666]\n",
            "Step  281407 [5.552 sec/step, loss=0.25178, avg_loss=0.26640]\n",
            "Step  281408 [5.559 sec/step, loss=0.26781, avg_loss=0.26633]\n",
            "Step  281409 [5.553 sec/step, loss=0.26970, avg_loss=0.26621]\n",
            "Step  281410 [5.549 sec/step, loss=0.26304, avg_loss=0.26610]\n",
            "Step  281411 [5.546 sec/step, loss=0.26295, avg_loss=0.26603]\n",
            "Step  281412 [5.551 sec/step, loss=0.27057, avg_loss=0.26606]\n",
            "Step  281413 [5.546 sec/step, loss=0.26777, avg_loss=0.26600]\n",
            "Step  281414 [5.551 sec/step, loss=0.26524, avg_loss=0.26599]\n",
            "Step  281415 [5.547 sec/step, loss=0.25348, avg_loss=0.26584]\n",
            "Step  281416 [5.533 sec/step, loss=0.25108, avg_loss=0.26562]\n",
            "Step  281417 [5.550 sec/step, loss=0.27877, avg_loss=0.26571]\n",
            "Step  281418 [5.551 sec/step, loss=0.26512, avg_loss=0.26566]\n",
            "Step  281419 [5.544 sec/step, loss=0.26424, avg_loss=0.26557]\n",
            "Step  281420 [5.552 sec/step, loss=0.26453, avg_loss=0.26567]\n",
            "Step  281421 [5.567 sec/step, loss=0.26787, avg_loss=0.26580]\n",
            "Step  281422 [5.565 sec/step, loss=0.27026, avg_loss=0.26581]\n",
            "Step  281423 [5.573 sec/step, loss=0.26968, avg_loss=0.26583]\n",
            "Step  281424 [5.578 sec/step, loss=0.26976, avg_loss=0.26594]\n",
            "Step  281425 [5.580 sec/step, loss=0.26750, avg_loss=0.26595]\n",
            "Step  281426 [5.573 sec/step, loss=0.26455, avg_loss=0.26594]\n",
            "Step  281427 [5.575 sec/step, loss=0.26119, avg_loss=0.26592]\n",
            "Step  281428 [5.577 sec/step, loss=0.26768, avg_loss=0.26601]\n",
            "Step  281429 [5.564 sec/step, loss=0.26409, avg_loss=0.26584]\n",
            "Step  281430 [5.561 sec/step, loss=0.26256, avg_loss=0.26572]\n",
            "Step  281431 [5.559 sec/step, loss=0.26990, avg_loss=0.26569]\n",
            "Step  281432 [5.003 sec/step, loss=0.26905, avg_loss=0.26583]\n",
            "Step  281433 [5.014 sec/step, loss=0.27847, avg_loss=0.26597]\n",
            "Step  281434 [5.002 sec/step, loss=0.26931, avg_loss=0.26587]\n",
            "Step  281435 [5.008 sec/step, loss=0.26207, avg_loss=0.26585]\n",
            "Step  281436 [4.995 sec/step, loss=0.25357, avg_loss=0.26559]\n",
            "Step  281437 [4.996 sec/step, loss=0.26414, avg_loss=0.26565]\n",
            "Step  281438 [4.996 sec/step, loss=0.26369, avg_loss=0.26559]\n",
            "Step  281439 [5.001 sec/step, loss=0.26723, avg_loss=0.26560]\n",
            "Step  281440 [4.992 sec/step, loss=0.26373, avg_loss=0.26554]\n",
            "Step  281441 [4.985 sec/step, loss=0.25473, avg_loss=0.26540]\n",
            "Step  281442 [4.990 sec/step, loss=0.27532, avg_loss=0.26551]\n",
            "Step  281443 [4.991 sec/step, loss=0.26364, avg_loss=0.26561]\n",
            "Step  281444 [4.994 sec/step, loss=0.26717, avg_loss=0.26559]\n",
            "Step  281445 [4.978 sec/step, loss=0.25109, avg_loss=0.26543]\n",
            "Step  281446 [4.977 sec/step, loss=0.26660, avg_loss=0.26531]\n",
            "Step  281447 [4.985 sec/step, loss=0.26692, avg_loss=0.26530]\n",
            "Step  281448 [4.996 sec/step, loss=0.25926, avg_loss=0.26536]\n",
            "Step  281449 [5.014 sec/step, loss=0.26706, avg_loss=0.26550]\n",
            "Step  281450 [5.004 sec/step, loss=0.26338, avg_loss=0.26544]\n",
            "Step  281451 [5.013 sec/step, loss=0.26967, avg_loss=0.26556]\n",
            "Step  281452 [5.011 sec/step, loss=0.26600, avg_loss=0.26543]\n",
            "Step  281453 [5.007 sec/step, loss=0.25341, avg_loss=0.26532]\n",
            "Step  281454 [5.003 sec/step, loss=0.26985, avg_loss=0.26533]\n",
            "Step  281455 [4.999 sec/step, loss=0.26295, avg_loss=0.26529]\n",
            "Step  281456 [5.006 sec/step, loss=0.27780, avg_loss=0.26540]\n",
            "Step  281457 [5.005 sec/step, loss=0.26127, avg_loss=0.26533]\n",
            "Step  281458 [5.020 sec/step, loss=0.27805, avg_loss=0.26543]\n",
            "Step  281459 [5.018 sec/step, loss=0.25030, avg_loss=0.26527]\n",
            "\n",
            "Generated 64 train batches of size 36 in 94.667 sec\n",
            "Step  281460 [5.545 sec/step, loss=0.26335, avg_loss=0.26524]\n",
            "Step  281461 [5.531 sec/step, loss=0.26321, avg_loss=0.26515]\n",
            "Step  281462 [5.538 sec/step, loss=0.26094, avg_loss=0.26518]\n",
            "Step  281463 [5.527 sec/step, loss=0.26299, avg_loss=0.26502]\n",
            "Step  281464 [5.529 sec/step, loss=0.25969, avg_loss=0.26488]\n",
            "Step  281465 [5.516 sec/step, loss=0.25976, avg_loss=0.26479]\n",
            "Step  281466 [5.514 sec/step, loss=0.26576, avg_loss=0.26476]\n",
            "Step  281467 [5.514 sec/step, loss=0.26599, avg_loss=0.26464]\n",
            "Step  281468 [5.497 sec/step, loss=0.26236, avg_loss=0.26458]\n",
            "Step  281469 [5.496 sec/step, loss=0.26855, avg_loss=0.26460]\n",
            "Step  281470 [5.508 sec/step, loss=0.27604, avg_loss=0.26470]\n",
            "Step  281471 [5.508 sec/step, loss=0.26790, avg_loss=0.26481]\n",
            "Step  281472 [5.510 sec/step, loss=0.26841, avg_loss=0.26477]\n",
            "Step  281473 [5.514 sec/step, loss=0.27655, avg_loss=0.26480]\n",
            "Step  281474 [5.509 sec/step, loss=0.26846, avg_loss=0.26481]\n",
            "Step  281475 [5.508 sec/step, loss=0.27623, avg_loss=0.26484]\n",
            "Step  281476 [5.505 sec/step, loss=0.26187, avg_loss=0.26474]\n",
            "Step  281477 [5.505 sec/step, loss=0.25077, avg_loss=0.26467]\n",
            "Step  281478 [5.501 sec/step, loss=0.25473, avg_loss=0.26449]\n",
            "Step  281479 [5.500 sec/step, loss=0.26682, avg_loss=0.26446]\n",
            "Step  281480 [5.510 sec/step, loss=0.26643, avg_loss=0.26455]\n",
            "Step  281481 [5.513 sec/step, loss=0.25997, avg_loss=0.26450]\n",
            "Step  281482 [5.505 sec/step, loss=0.26315, avg_loss=0.26448]\n",
            "Step  281483 [5.514 sec/step, loss=0.26451, avg_loss=0.26446]\n",
            "Step  281484 [5.505 sec/step, loss=0.24894, avg_loss=0.26431]\n",
            "Step  281485 [5.505 sec/step, loss=0.25024, avg_loss=0.26428]\n",
            "Step  281486 [5.519 sec/step, loss=0.26575, avg_loss=0.26422]\n",
            "Step  281487 [5.508 sec/step, loss=0.26305, avg_loss=0.26417]\n",
            "Step  281488 [5.497 sec/step, loss=0.25364, avg_loss=0.26400]\n",
            "Step  281489 [5.502 sec/step, loss=0.26543, avg_loss=0.26401]\n",
            "Step  281490 [5.510 sec/step, loss=0.27249, avg_loss=0.26407]\n",
            "Step  281491 [5.508 sec/step, loss=0.26283, avg_loss=0.26406]\n",
            "Step  281492 [5.509 sec/step, loss=0.26882, avg_loss=0.26418]\n",
            "Step  281493 [5.508 sec/step, loss=0.26564, avg_loss=0.26413]\n",
            "Step  281494 [5.510 sec/step, loss=0.25384, avg_loss=0.26416]\n",
            "Step  281495 [5.512 sec/step, loss=0.26462, avg_loss=0.26417]\n",
            "Step  281496 [4.961 sec/step, loss=0.25261, avg_loss=0.26405]\n",
            "Step  281497 [4.961 sec/step, loss=0.25334, avg_loss=0.26402]\n",
            "Step  281498 [4.967 sec/step, loss=0.26156, avg_loss=0.26411]\n",
            "Step  281499 [4.955 sec/step, loss=0.26831, avg_loss=0.26401]\n",
            "Step  281500 [4.967 sec/step, loss=0.25981, avg_loss=0.26410]\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 281500: a graduation ceremony doesn't celebrate just a moment in time, it's the culmination of all your years of learning about the world and about yourself.~\n",
            "Step  281501 [4.963 sec/step, loss=0.26814, avg_loss=0.26408]\n",
            "Step  281502 [4.966 sec/step, loss=0.27524, avg_loss=0.26415]\n",
            "Step  281503 [4.973 sec/step, loss=0.26142, avg_loss=0.26421]\n",
            "Step  281504 [4.969 sec/step, loss=0.25211, avg_loss=0.26409]\n",
            "Step  281505 [4.969 sec/step, loss=0.26654, avg_loss=0.26411]\n",
            "Step  281506 [4.968 sec/step, loss=0.26281, avg_loss=0.26418]\n",
            "Step  281507 [4.974 sec/step, loss=0.26462, avg_loss=0.26431]\n",
            "Step  281508 [4.960 sec/step, loss=0.24985, avg_loss=0.26413]\n",
            "Step  281509 [4.954 sec/step, loss=0.25211, avg_loss=0.26396]\n",
            "Step  281510 [4.960 sec/step, loss=0.26462, avg_loss=0.26397]\n",
            "Step  281511 [4.952 sec/step, loss=0.24951, avg_loss=0.26384]\n",
            "Step  281512 [4.953 sec/step, loss=0.26258, avg_loss=0.26376]\n",
            "Step  281513 [4.949 sec/step, loss=0.26636, avg_loss=0.26374]\n",
            "Step  281514 [4.956 sec/step, loss=0.26069, avg_loss=0.26370]\n",
            "Step  281515 [4.966 sec/step, loss=0.26510, avg_loss=0.26382]\n",
            "Step  281516 [4.986 sec/step, loss=0.27533, avg_loss=0.26406]\n",
            "Step  281517 [4.971 sec/step, loss=0.26295, avg_loss=0.26390]\n",
            "Step  281518 [4.978 sec/step, loss=0.26714, avg_loss=0.26392]\n",
            "Step  281519 [4.985 sec/step, loss=0.25953, avg_loss=0.26387]\n",
            "Step  281520 [4.982 sec/step, loss=0.26195, avg_loss=0.26385]\n",
            "Step  281521 [4.980 sec/step, loss=0.26418, avg_loss=0.26381]\n",
            "Step  281522 [4.979 sec/step, loss=0.24885, avg_loss=0.26360]\n",
            "\n",
            "Generated 64 train batches of size 36 in 66.968 sec\n",
            "Step  281523 [5.218 sec/step, loss=0.26375, avg_loss=0.26354]\n",
            "Step  281524 [5.213 sec/step, loss=0.26117, avg_loss=0.26345]\n",
            "Step  281525 [5.213 sec/step, loss=0.26718, avg_loss=0.26345]\n",
            "Step  281526 [5.214 sec/step, loss=0.24844, avg_loss=0.26329]\n",
            "Step  281527 [5.217 sec/step, loss=0.26647, avg_loss=0.26334]\n",
            "Step  281528 [5.220 sec/step, loss=0.27476, avg_loss=0.26341]\n",
            "Step  281529 [5.222 sec/step, loss=0.25946, avg_loss=0.26336]\n",
            "Step  281530 [5.227 sec/step, loss=0.25796, avg_loss=0.26332]\n",
            "Step  281531 [5.220 sec/step, loss=0.25286, avg_loss=0.26315]\n",
            "Step  281532 [5.218 sec/step, loss=0.24600, avg_loss=0.26292]\n",
            "Step  281533 [5.210 sec/step, loss=0.26693, avg_loss=0.26280]\n",
            "Step  281534 [5.199 sec/step, loss=0.25209, avg_loss=0.26263]\n",
            "Step  281535 [5.203 sec/step, loss=0.26518, avg_loss=0.26266]\n",
            "Step  281536 [5.215 sec/step, loss=0.26349, avg_loss=0.26276]\n",
            "Step  281537 [5.224 sec/step, loss=0.26534, avg_loss=0.26277]\n",
            "Step  281538 [5.224 sec/step, loss=0.26437, avg_loss=0.26278]\n",
            "Step  281539 [5.230 sec/step, loss=0.27460, avg_loss=0.26285]\n",
            "Step  281540 [5.234 sec/step, loss=0.25919, avg_loss=0.26281]\n",
            "Step  281541 [5.245 sec/step, loss=0.25888, avg_loss=0.26285]\n",
            "Step  281542 [5.234 sec/step, loss=0.26598, avg_loss=0.26275]\n",
            "Step  281543 [5.247 sec/step, loss=0.26379, avg_loss=0.26276]\n",
            "Step  281544 [5.239 sec/step, loss=0.26558, avg_loss=0.26274]\n",
            "Step  281545 [5.253 sec/step, loss=0.26458, avg_loss=0.26288]\n",
            "Step  281546 [5.243 sec/step, loss=0.26192, avg_loss=0.26283]\n",
            "Step  281547 [5.243 sec/step, loss=0.26329, avg_loss=0.26279]\n",
            "Step  281548 [5.229 sec/step, loss=0.25067, avg_loss=0.26271]\n",
            "Step  281549 [5.214 sec/step, loss=0.26276, avg_loss=0.26266]\n",
            "Step  281550 [5.214 sec/step, loss=0.26130, avg_loss=0.26264]\n",
            "Step  281551 [5.199 sec/step, loss=0.26186, avg_loss=0.26256]\n",
            "Step  281552 [5.193 sec/step, loss=0.26537, avg_loss=0.26256]\n",
            "Step  281553 [5.196 sec/step, loss=0.26110, avg_loss=0.26264]\n",
            "Step  281554 [5.192 sec/step, loss=0.26347, avg_loss=0.26257]\n",
            "Step  281555 [5.200 sec/step, loss=0.25883, avg_loss=0.26253]\n",
            "Step  281556 [5.196 sec/step, loss=0.27179, avg_loss=0.26247]\n",
            "Step  281557 [5.180 sec/step, loss=0.25149, avg_loss=0.26237]\n",
            "Step  281558 [5.166 sec/step, loss=0.25785, avg_loss=0.26217]\n",
            "Step  281559 [5.169 sec/step, loss=0.26093, avg_loss=0.26228]\n",
            "Step  281560 [4.651 sec/step, loss=0.25565, avg_loss=0.26220]\n",
            "Step  281561 [4.656 sec/step, loss=0.26272, avg_loss=0.26219]\n",
            "Step  281562 [4.649 sec/step, loss=0.26273, avg_loss=0.26221]\n",
            "Step  281563 [4.661 sec/step, loss=0.27426, avg_loss=0.26233]\n",
            "Step  281564 [4.659 sec/step, loss=0.25994, avg_loss=0.26233]\n",
            "Step  281565 [4.668 sec/step, loss=0.26325, avg_loss=0.26236]\n",
            "Step  281566 [4.671 sec/step, loss=0.26304, avg_loss=0.26234]\n",
            "Step  281567 [4.652 sec/step, loss=0.26017, avg_loss=0.26228]\n",
            "Step  281568 [4.652 sec/step, loss=0.24861, avg_loss=0.26214]\n",
            "Step  281569 [4.650 sec/step, loss=0.26271, avg_loss=0.26208]\n",
            "Step  281570 [4.635 sec/step, loss=0.25015, avg_loss=0.26182]\n",
            "Step  281571 [4.645 sec/step, loss=0.26279, avg_loss=0.26177]\n",
            "Step  281572 [4.638 sec/step, loss=0.24807, avg_loss=0.26157]\n",
            "Step  281573 [4.631 sec/step, loss=0.26132, avg_loss=0.26142]\n",
            "Step  281574 [4.628 sec/step, loss=0.25943, avg_loss=0.26133]\n",
            "Step  281575 [4.623 sec/step, loss=0.25984, avg_loss=0.26116]\n",
            "Step  281576 [4.634 sec/step, loss=0.27402, avg_loss=0.26128]\n",
            "Step  281577 [4.631 sec/step, loss=0.25042, avg_loss=0.26128]\n",
            "Step  281578 [4.633 sec/step, loss=0.25042, avg_loss=0.26124]\n",
            "Step  281579 [4.629 sec/step, loss=0.26258, avg_loss=0.26119]\n",
            "Step  281580 [4.623 sec/step, loss=0.24797, avg_loss=0.26101]\n",
            "Step  281581 [4.617 sec/step, loss=0.24712, avg_loss=0.26088]\n",
            "Step  281582 [4.628 sec/step, loss=0.26556, avg_loss=0.26090]\n",
            "Step  281583 [4.627 sec/step, loss=0.26057, avg_loss=0.26087]\n",
            "Step  281584 [4.639 sec/step, loss=0.26230, avg_loss=0.26100]\n",
            "Step  281585 [4.642 sec/step, loss=0.26533, avg_loss=0.26115]\n",
            "Step  281586 [4.639 sec/step, loss=0.27393, avg_loss=0.26123]\n",
            "\n",
            "Generated 64 train batches of size 36 in 89.164 sec\n",
            "Step  281587 [5.151 sec/step, loss=0.26184, avg_loss=0.26122]\n",
            "Step  281588 [5.158 sec/step, loss=0.25879, avg_loss=0.26127]\n",
            "Step  281589 [5.155 sec/step, loss=0.26224, avg_loss=0.26124]\n",
            "Step  281590 [5.150 sec/step, loss=0.25687, avg_loss=0.26108]\n",
            "Step  281591 [5.159 sec/step, loss=0.27312, avg_loss=0.26119]\n",
            "Step  281592 [5.159 sec/step, loss=0.26533, avg_loss=0.26115]\n",
            "Step  281593 [5.164 sec/step, loss=0.26211, avg_loss=0.26112]\n",
            "Step  281594 [5.170 sec/step, loss=0.27251, avg_loss=0.26130]\n",
            "Step  281595 [5.158 sec/step, loss=0.25102, avg_loss=0.26117]\n",
            "Step  281596 [5.152 sec/step, loss=0.24705, avg_loss=0.26111]\n",
            "Step  281597 [5.160 sec/step, loss=0.25761, avg_loss=0.26115]\n",
            "Step  281598 [5.153 sec/step, loss=0.24623, avg_loss=0.26100]\n",
            "Step  281599 [5.153 sec/step, loss=0.26107, avg_loss=0.26093]\n",
            "Step  281600 [5.151 sec/step, loss=0.26195, avg_loss=0.26095]\n",
            "Step  281601 [5.161 sec/step, loss=0.26217, avg_loss=0.26089]\n",
            "Step  281602 [5.160 sec/step, loss=0.25718, avg_loss=0.26071]\n",
            "Step  281603 [5.161 sec/step, loss=0.27216, avg_loss=0.26082]\n",
            "Step  281604 [5.159 sec/step, loss=0.24989, avg_loss=0.26079]\n",
            "Step  281605 [5.153 sec/step, loss=0.26013, avg_loss=0.26073]\n",
            "Step  281606 [5.149 sec/step, loss=0.25964, avg_loss=0.26070]\n",
            "Step  281607 [5.137 sec/step, loss=0.24912, avg_loss=0.26054]\n",
            "Step  281608 [5.145 sec/step, loss=0.26167, avg_loss=0.26066]\n",
            "Step  281609 [5.146 sec/step, loss=0.24411, avg_loss=0.26058]\n",
            "Step  281610 [5.141 sec/step, loss=0.26593, avg_loss=0.26059]\n",
            "Step  281611 [5.142 sec/step, loss=0.25875, avg_loss=0.26069]\n",
            "Step  281612 [5.140 sec/step, loss=0.26093, avg_loss=0.26067]\n",
            "Step  281613 [5.137 sec/step, loss=0.25978, avg_loss=0.26061]\n",
            "Step  281614 [5.120 sec/step, loss=0.24516, avg_loss=0.26045]\n",
            "Step  281615 [5.112 sec/step, loss=0.26574, avg_loss=0.26046]\n",
            "Step  281616 [5.101 sec/step, loss=0.26125, avg_loss=0.26032]\n",
            "Step  281617 [5.096 sec/step, loss=0.26549, avg_loss=0.26034]\n",
            "Step  281618 [5.100 sec/step, loss=0.27143, avg_loss=0.26038]\n",
            "Step  281619 [5.093 sec/step, loss=0.25935, avg_loss=0.26038]\n",
            "Step  281620 [5.094 sec/step, loss=0.25858, avg_loss=0.26035]\n",
            "Step  281621 [5.080 sec/step, loss=0.25916, avg_loss=0.26030]\n",
            "Step  281622 [5.079 sec/step, loss=0.25943, avg_loss=0.26040]\n",
            "Step  281623 [4.836 sec/step, loss=0.25927, avg_loss=0.26036]\n",
            "Step  281624 [4.835 sec/step, loss=0.24846, avg_loss=0.26023]\n",
            "Step  281625 [4.830 sec/step, loss=0.25866, avg_loss=0.26015]\n",
            "Step  281626 [4.833 sec/step, loss=0.25581, avg_loss=0.26022]\n",
            "Step  281627 [4.827 sec/step, loss=0.26045, avg_loss=0.26016]\n",
            "Step  281628 [4.827 sec/step, loss=0.25768, avg_loss=0.25999]\n",
            "Step  281629 [4.827 sec/step, loss=0.25606, avg_loss=0.25996]\n",
            "Step  281630 [4.820 sec/step, loss=0.24510, avg_loss=0.25983]\n",
            "Step  281631 [4.821 sec/step, loss=0.24780, avg_loss=0.25978]\n",
            "Step  281632 [4.825 sec/step, loss=0.26418, avg_loss=0.25996]\n",
            "Step  281633 [4.822 sec/step, loss=0.26627, avg_loss=0.25995]\n",
            "Step  281634 [4.840 sec/step, loss=0.26086, avg_loss=0.26004]\n",
            "Step  281635 [4.824 sec/step, loss=0.24483, avg_loss=0.25984]\n",
            "Step  281636 [4.821 sec/step, loss=0.25882, avg_loss=0.25979]\n",
            "Step  281637 [4.821 sec/step, loss=0.27135, avg_loss=0.25985]\n",
            "Step  281638 [4.818 sec/step, loss=0.24832, avg_loss=0.25969]\n",
            "Step  281639 [4.810 sec/step, loss=0.26039, avg_loss=0.25955]\n",
            "Step  281640 [4.810 sec/step, loss=0.26572, avg_loss=0.25961]\n",
            "Step  281641 [4.807 sec/step, loss=0.26083, avg_loss=0.25963]\n",
            "Step  281642 [4.826 sec/step, loss=0.26906, avg_loss=0.25966]\n",
            "Step  281643 [4.823 sec/step, loss=0.26371, avg_loss=0.25966]\n",
            "Step  281644 [4.826 sec/step, loss=0.26568, avg_loss=0.25966]\n",
            "Step  281645 [4.825 sec/step, loss=0.25622, avg_loss=0.25958]\n",
            "Step  281646 [4.826 sec/step, loss=0.25758, avg_loss=0.25954]\n",
            "Step  281647 [4.835 sec/step, loss=0.26245, avg_loss=0.25953]\n",
            "Step  281648 [4.846 sec/step, loss=0.25643, avg_loss=0.25958]\n",
            "Step  281649 [4.843 sec/step, loss=0.24790, avg_loss=0.25944]\n",
            "Step  281650 [4.849 sec/step, loss=0.26424, avg_loss=0.25947]\n",
            "\n",
            "Generated 64 train batches of size 36 in 85.857 sec\n",
            "Step  281651 [5.294 sec/step, loss=0.25595, avg_loss=0.25941]\n",
            "Step  281652 [5.291 sec/step, loss=0.24523, avg_loss=0.25920]\n",
            "Step  281653 [5.302 sec/step, loss=0.26077, avg_loss=0.25920]\n",
            "Step  281654 [5.293 sec/step, loss=0.24467, avg_loss=0.25901]\n",
            "Step  281655 [5.282 sec/step, loss=0.25776, avg_loss=0.25900]\n",
            "Step  281656 [5.283 sec/step, loss=0.26097, avg_loss=0.25889]\n",
            "Step  281657 [5.295 sec/step, loss=0.26092, avg_loss=0.25899]\n",
            "Step  281658 [5.295 sec/step, loss=0.24222, avg_loss=0.25883]\n",
            "Step  281659 [5.303 sec/step, loss=0.26316, avg_loss=0.25885]\n",
            "Step  281660 [5.300 sec/step, loss=0.26378, avg_loss=0.25894]\n",
            "Step  281661 [5.300 sec/step, loss=0.25845, avg_loss=0.25889]\n",
            "Step  281662 [5.301 sec/step, loss=0.25786, avg_loss=0.25884]\n",
            "Step  281663 [5.293 sec/step, loss=0.25847, avg_loss=0.25869]\n",
            "Step  281664 [5.293 sec/step, loss=0.26354, avg_loss=0.25872]\n",
            "Step  281665 [5.291 sec/step, loss=0.26361, avg_loss=0.25873]\n",
            "Step  281666 [5.290 sec/step, loss=0.27142, avg_loss=0.25881]\n",
            "Step  281667 [5.295 sec/step, loss=0.24789, avg_loss=0.25869]\n",
            "Step  281668 [5.307 sec/step, loss=0.26194, avg_loss=0.25882]\n",
            "Step  281669 [5.299 sec/step, loss=0.25661, avg_loss=0.25876]\n",
            "Step  281670 [5.302 sec/step, loss=0.25765, avg_loss=0.25883]\n",
            "Step  281671 [5.306 sec/step, loss=0.26037, avg_loss=0.25881]\n",
            "Step  281672 [5.302 sec/step, loss=0.24330, avg_loss=0.25876]\n",
            "Step  281673 [5.309 sec/step, loss=0.25980, avg_loss=0.25875]\n",
            "Step  281674 [5.319 sec/step, loss=0.27164, avg_loss=0.25887]\n",
            "Step  281675 [5.327 sec/step, loss=0.26031, avg_loss=0.25887]\n",
            "Step  281676 [5.320 sec/step, loss=0.25765, avg_loss=0.25871]\n",
            "Step  281677 [5.331 sec/step, loss=0.25601, avg_loss=0.25877]\n",
            "Step  281678 [5.343 sec/step, loss=0.25967, avg_loss=0.25886]\n",
            "Step  281679 [5.346 sec/step, loss=0.26097, avg_loss=0.25884]\n",
            "Step  281680 [5.348 sec/step, loss=0.25706, avg_loss=0.25893]\n",
            "Step  281681 [5.359 sec/step, loss=0.26710, avg_loss=0.25913]\n",
            "Step  281682 [5.353 sec/step, loss=0.24279, avg_loss=0.25891]\n",
            "Step  281683 [5.358 sec/step, loss=0.25303, avg_loss=0.25883]\n",
            "Step  281684 [5.361 sec/step, loss=0.25900, avg_loss=0.25880]\n",
            "Step  281685 [5.370 sec/step, loss=0.25895, avg_loss=0.25873]\n",
            "Step  281686 [5.361 sec/step, loss=0.25698, avg_loss=0.25856]\n",
            "Step  281687 [4.851 sec/step, loss=0.26128, avg_loss=0.25856]\n",
            "Step  281688 [4.856 sec/step, loss=0.25611, avg_loss=0.25853]\n",
            "Step  281689 [4.849 sec/step, loss=0.25549, avg_loss=0.25846]\n",
            "Step  281690 [4.850 sec/step, loss=0.25524, avg_loss=0.25845]\n",
            "Step  281691 [4.842 sec/step, loss=0.24850, avg_loss=0.25820]\n",
            "Step  281692 [4.839 sec/step, loss=0.24807, avg_loss=0.25803]\n",
            "Step  281693 [4.845 sec/step, loss=0.27100, avg_loss=0.25812]\n",
            "Step  281694 [4.849 sec/step, loss=0.27116, avg_loss=0.25810]\n",
            "Step  281695 [4.851 sec/step, loss=0.25634, avg_loss=0.25816]\n",
            "Step  281696 [4.855 sec/step, loss=0.26275, avg_loss=0.25831]\n",
            "Step  281697 [4.858 sec/step, loss=0.26259, avg_loss=0.25836]\n",
            "Step  281698 [4.861 sec/step, loss=0.25630, avg_loss=0.25846]\n",
            "Step  281699 [4.861 sec/step, loss=0.25720, avg_loss=0.25843]\n",
            "Step  281700 [4.864 sec/step, loss=0.25933, avg_loss=0.25840]\n",
            "Step  281701 [4.852 sec/step, loss=0.24760, avg_loss=0.25825]\n",
            "Step  281702 [4.841 sec/step, loss=0.25371, avg_loss=0.25822]\n",
            "Step  281703 [4.842 sec/step, loss=0.24839, avg_loss=0.25798]\n",
            "Step  281704 [4.859 sec/step, loss=0.26991, avg_loss=0.25818]\n",
            "Step  281705 [4.857 sec/step, loss=0.24817, avg_loss=0.25806]\n",
            "Step  281706 [4.862 sec/step, loss=0.24670, avg_loss=0.25793]\n",
            "Step  281707 [4.867 sec/step, loss=0.25719, avg_loss=0.25801]\n",
            "Step  281708 [4.867 sec/step, loss=0.26219, avg_loss=0.25802]\n",
            "Step  281709 [4.880 sec/step, loss=0.25881, avg_loss=0.25817]\n",
            "Step  281710 [4.879 sec/step, loss=0.25705, avg_loss=0.25808]\n",
            "Step  281711 [4.885 sec/step, loss=0.25966, avg_loss=0.25809]\n",
            "Step  281712 [4.886 sec/step, loss=0.25571, avg_loss=0.25803]\n",
            "Step  281713 [4.895 sec/step, loss=0.24321, avg_loss=0.25787]\n",
            "Step  281714 [4.911 sec/step, loss=0.26240, avg_loss=0.25804]\n",
            "\n",
            "Generated 64 train batches of size 36 in 90.593 sec\n",
            "2021-02-11 13:48:37.647676: W tensorflow/core/kernels/queue_base.cc:277] _0_datafeeder/input_queue: Skipping cancelled enqueue attempt with queue not closed\n",
            "Traceback (most recent call last):\n",
            "  File \"synthesizer_train.py\", line 55, in <module>\n",
            "    tacotron_train(args, log_dir, hparams)\n",
            "  File \"/content/gdrive/My Drive/Real-Time-Voice-Cloning/synthesizer/train.py\", line 392, in tacotron_train\n",
            "    return train(log_dir, args, hparams)\n",
            "  File \"/content/gdrive/My Drive/Real-Time-Voice-Cloning/synthesizer/train.py\", line 210, in train\n",
            "    step, loss, opt = sess.run([global_step, model.loss, model.optimize])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCA8E-9_PgJT"
      },
      "source": [
        "!python vocoder_preprocess.py --model_dir synthesizer/saved_models/logs-blue-fish-pre/ --no_trim datasets_root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLiVhTxPjOln"
      },
      "source": [
        "!python vocoder_train.py blue-fish-pre datasets_root"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}