{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP17vfjI1Ly95RXeaj3P8em"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"efyX7EiP61Eo","colab_type":"code","outputId":"7159bf15-106e-48de-900f-5d35d7c32827","executionInfo":{"status":"ok","timestamp":1590488812799,"user_tz":-330,"elapsed":7463,"user":{"displayName":"Comp4711 Project","photoUrl":"","userId":"06130510692457946417"}},"colab":{"base_uri":"https://localhost:8080/","height":356}},"source":["!pip install face-alignment"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: face-alignment in /usr/local/lib/python3.6/dist-packages (1.0.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from face-alignment) (1.5.0+cu101)\n","Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from face-alignment) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from face-alignment) (4.41.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from face-alignment) (4.1.2.30)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face-alignment) (1.18.4)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from face-alignment) (0.16.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->face-alignment) (0.16.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face-alignment) (3.2.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face-alignment) (2.4)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face-alignment) (2.4.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face-alignment) (1.1.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face-alignment) (7.0.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (2.4.7)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->face-alignment) (4.4.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JNsgMXjV6NrI","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import math\n","import os\n","import sys\n","#from dataset import MyDataset\n","import numpy as np\n","import time\n","#from model import LipNet\n","import torch.optim as optim\n","import re\n","import json\n","import tempfile\n","import shutil\n","import cv2\n","import face_alignment"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yer62RgiMN9K","colab_type":"code","outputId":"00a9f74c-ad85-4372-a5bf-713865d66822","executionInfo":{"status":"ok","timestamp":1590488812803,"user_tz":-330,"elapsed":6863,"user":{"displayName":"Comp4711 Project","photoUrl":"","userId":"06130510692457946417"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My\\ Drive/Comp4471\\ Project/Evaluation"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Comp4471 Project/Evaluation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uVawI9noGskB","colab_type":"code","colab":{}},"source":["# Identify the file for loading the input video. Put your file in the folder evaluation_vids and change file_name to the \n","# corresponding file's name\n","\n","file_name = '2.mp4'\n","file_path = 'evaluation_vids/' + file_name"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TobpKxXULJJl","colab_type":"code","colab":{}},"source":["class LipNet(torch.nn.Module):\n","# Define the structure of the Lipnet module as given in the paper of LipNet.\n","# This will be loaded with the pretrained weights in the evaluation script\n","    \n","    def __init__(self, drop_prob =0.5):\n","\n","        self.dropout_probability  = drop_prob\n","        super(LipNet, self).__init__()\n","        self.conv1 = nn.Conv3d(3, 32, (3, 5, 5), (1, 2, 2), (1, 2, 2))\n","        self.pool1 = nn.MaxPool3d((1, 2, 2), (1, 2, 2))\n","        \n","        self.conv2 = nn.Conv3d(32, 64, (3, 5, 5), (1, 1, 1), (1, 2, 2))  # Original paper says (32, 64, (3, 5, 5), (1, 2, 2), (1, 2, 2))\n","        self.pool2 = nn.MaxPool3d((1, 2, 2), (1, 2, 2))\n","        \n","        self.conv3 = nn.Conv3d(64, 96, (3, 3, 3), (1, 1, 1), (1, 1, 1))     \n","        self.pool3 = nn.MaxPool3d((1, 2, 2), (1, 2, 2))\n","        \n","        self.gru1  = nn.GRU(96*4*8, 256, 1, bidirectional=True)\n","        self.gru2  = nn.GRU(512, 256, 1, bidirectional=True)\n","        \n","        self.FC    = nn.Linear(512, 27+1)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.dropout = nn.Dropout(self.dropout_probability)        \n","        self.dropout3d = nn.Dropout3d(self.dropout_probability)  \n","\n","    def forward(self, x):\n","        \n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        x = self.dropout3d(x)\n","        x = self.pool1(x)\n","        \n","        x = self.conv2(x)\n","        x = self.relu(x)\n","        x = self.dropout3d(x)        \n","        x = self.pool2(x)\n","        \n","        x = self.conv3(x)\n","        x = self.relu(x)\n","        x = self.dropout3d(x)        \n","        x = self.pool3(x)\n","        \n","        x = x.permute(2, 0, 1, 3, 4).contiguous()\n","        x = x.view(x.size(0), x.size(1), -1)\n","        \n","        self.gru1.flatten_parameters()\n","        self.gru2.flatten_parameters()\n","        \n","        x, h = self.gru1(x)        \n","        x = self.dropout(x)\n","        x, h = self.gru2(x)   \n","        x = self.dropout(x)\n","                \n","        x = self.FC(x)\n","        x = x.permute(1, 0, 2).contiguous()\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1Kg44AJ5fW4","colab_type":"code","colab":{}},"source":["def load_video(video_file):\n","    # Loads the video file using cv2 Video Capture, as a list of frames. Iterated over the frames \n","    # and extracts the lip region for each frame. Finally, returns a list of the lip extracted\n","    # frames from the video as output, which will be the input to the LipNet\n","    \n","    # Get the frames from the video\n","    cam = cv2.VideoCapture(video_file)\n","    array = []\n","    while(True): \n","      ret,frame = cam.read()\n","      if ret:\n","        array.append(frame)\n","      else:\n","        break\n","    \n","    array = list(filter(lambda im: not im is None, array))\n","    \n","    # Extract the facial landmarks from each frame in array\n","    face_landmarks = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False, device='cuda')\n","    landmarks = [face_landmarks.get_landmarks(I) for I in array]\n","    final_output = []\n","\n","    # Iterate over the frames, extract the lip region using coordinates from the face landmarks,\n","    # append the lip frame (resized to 128x64 for lipnet input) to the final output\n","    for ld, face in zip(landmarks, array):\n","        if(ld is not None):\n","            \n","            shape = np.array(ld[0])\n","            start_hor = int(shape[5][0])\n","            end_hor = int(shape[13][0])\n","            start_ver = int(np.minimum(shape[5][1], shape[13][1]))\n","            end_ver = int(shape[9][1])\n","\n","            img = face[start_ver:end_ver, start_hor:end_hor]\n","            img = cv2.resize(img, (128, 64), interpolation=cv2.INTER_LANCZOS4)\n","            final_output.append(img)\n","\n","    \n","    final_output = np.stack(final_output, axis=0).astype(np.float32)\n","    final_output = torch.FloatTensor(final_output.transpose(3, 0, 1, 2)) / 255.0\n","\n","    return final_output\n","\n","\n","def output_to_text(output, start_index):\n","# Takes in the digits of the output from lipnet and converts it into characters. Returns the text output\n","\n","    characters = [' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n","    check = -1\n","    final_output = []\n","    for i in output:\n","        if(check != i and i >= start_index):                \n","            if(len(final_output) > 0 and final_output[-1] == ' ' and characters[i - start_index] == ' '):\n","                pass\n","            else:\n","                final_output.append(characters[i - start_index])                \n","        check = i\n","    return ''.join(final_output).strip()\n","\n","def longest_common_substring(s1, s2):\n","   # Find the longest substring common between 2 given strings using dynamic programming\n","   # Used as a helper function for preprocess_text\n","   \n","   dp_matrix = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n","   length_longest, end_index = 0, 0\n","   for x in range(1, 1 + len(s1)):\n","       for y in range(1, 1 + len(s2)):\n","           if s1[x - 1] == s2[y - 1]:\n","               dp_matrix[x][y] = dp_matrix[x - 1][y - 1] + 1\n","               if dp_matrix[x][y] > length_longest:\n","                   length_longest = dp_matrix[x][y]\n","                   end_index = x\n","           else:\n","               dp_matrix[x][y] = 0\n","   return s1[end_index - length_longest: end_index]\n","\n","def preprocess_text(output_text):\n","  # Takes in the input text, and uses the longest common substring between the GRID vocabulary\n","  # to process the text according to the GRID vocabulary. Returns the list of words in the \n","  # processed text\n","\n","  list_words = output_text.split(\" \")\n","  word_1_list = ['BIN', 'LAY', 'PLACE', 'SET']\n","  word_2_list = ['BLUE', 'GREEN', 'RED', 'WHITE']\n","  word_3_list = ['AT', 'BY', 'IN', 'WITH']\n","  word_5_list = ['ONE', 'TWO', 'THREE', 'FOUR', 'FIVE', 'SIX', 'SEVEN', 'EIGHT', 'NINE', 'ZERO']\n","  word_6_list = ['AGAIN', 'NOW', 'PLEASE', 'SOON']\n","\n","  for i, item in enumerate(list_words):\n","\n","    if i == 0:\n","      lengths = list(map(lambda x: len(longest_common_substring(list_words[i], x)), word_1_list))\n","      highest_score = np.max(lengths)\n","      items = []\n","      for j, item in enumerate(word_1_list):\n","        if lengths[j] == highest_score:\n","          if item[0] == list_words[i][0]:\n","            items.append(item)\n","\n","      if len(items) == 0:\n","        for j, item in enumerate(word_6_list):\n","          if lengths[j] == highest_score:\n","            list_words[i] = item\n","      else:\n","        list_words[i] = items[0]\n","\n","    if i == 1:\n","      lengths = list(map(lambda x: len(longest_common_substring(list_words[i], x)), word_2_list))\n","      highest_score = np.max(lengths)\n","      items = []\n","      for j, item in enumerate(word_2_list):\n","        if lengths[j] == highest_score:\n","          if item[0] == list_words[i][0]:\n","            items.append(item)\n","\n","      if len(items) == 0:\n","        for j, item in enumerate(word_6_list):\n","          if lengths[j] == highest_score:\n","            list_words[i] = item\n","      else:\n","        list_words[i] = items[0]\n","\n","    if i == 2:\n","      lengths = list(map(lambda x: len(longest_common_substring(list_words[i], x)), word_3_list))\n","      highest_score = np.max(lengths)\n","      items = []\n","      for j, item in enumerate(word_3_list):\n","        if lengths[j] == highest_score:\n","          if item[0] == list_words[i][0]:\n","            items.append(item)\n","\n","      if len(items) == 0:\n","        for j, item in enumerate(word_6_list):\n","          if lengths[j] == highest_score:\n","            list_words[i] = item\n","      else:\n","        list_words[i] = items[0]\n","\n","    if i == 4:\n","      lengths = list(map(lambda x: len(longest_common_substring(list_words[i], x)), word_5_list))\n","      highest_score = np.max(lengths)\n","      items = []\n","\n","      for j, item in enumerate(word_5_list):\n","        if lengths[j] == highest_score:\n","          if item[0] == list_words[i][0]:\n","            items.append(item)\n","\n","      if len(items) == 0:\n","        for j, item in enumerate(word_6_list):\n","          if lengths[j] == highest_score:\n","            list_words[i] = item\n","      else:\n","        list_words[i] = items[0]\n","\n","    if i == 5:\n","      lengths = list(map(lambda x: len(longest_common_substring(list_words[i], x)), word_6_list))\n","      highest_score = np.max(lengths)\n","      items = []\n","      for j, item in enumerate(word_6_list):\n","        if lengths[j] == highest_score:\n","          if item[0] == list_words[i][0]:\n","            items.append(item)\n","\n","      if len(items) == 0:\n","        for j, item in enumerate(word_6_list):\n","          if lengths[j] == highest_score:\n","            list_words[i] = item\n","      else:\n","        list_words[i] = items[0]\n","   \n","  \n","  return list_words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RXiRwaS0VhQv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"6f067a46-1189-41c9-81f3-efee7ca1643a","executionInfo":{"status":"ok","timestamp":1590488819457,"user_tz":-330,"elapsed":9424,"user":{"displayName":"Comp4711 Project","photoUrl":"","userId":"06130510692457946417"}}},"source":["# Evaluation script\n","model = LipNet()\n","model = model.cuda()\n","pretrained_dict = torch.load('LipNet_unseen_loss_0.44562849402427673_wer_0.1332580699113564_cer_0.06796452465503355.pt')\n","model_dict = model.state_dict()\n","pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict.keys() and v.size() == model_dict[k].size()}\n","missed_params = [k for k, v in model_dict.items() if not k in pretrained_dict.keys()]\n","model_dict.update(pretrained_dict)\n","model.load_state_dict(model_dict)\n","    \n","video = load_video(file_path)\n","y = model(video[None,...].cuda())\n","y = y.cpu().numpy().squeeze()\n","y_args = np.argmax(y, axis = 1).astype('int8')\n","\n","txt = output_to_text(y_args, 1)\n","\n","processed_words = preprocess_text(txt)\n","\n","print(\"Ground Truth: LAY BLUE WITH B EIGHT PLEASE\")\n","print(\"Result of Lipnet Evaluation:\", processed_words)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Ground Truth: LAY BLUE WITH B EIGHT PLEASE\n","Result of Lipnet Evaluation: ['LAY', 'BLUE', 'WITH', 'V', 'TWO', 'PLEASE']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zAwmOnAhWHyM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}